We selected a cost measure which takes into account the flop count for the most costly functions identified by profiling the code.
The flop counts were done bottom up.

Cost measure:
C(N,D,V,K) = ( simple(N,D,V,K), heavy(N,D,V,K) )

simple(N,D,V,K): adds, muls
heavy(N,D,V,K): divs, logs, exps, etc.

Where;
    K is the number of topics
    N is the number of documents
    V is the size of the vocabulary
    D is the average length of a document
    var_iter_inf is the #iterations until convergence of the E-step
    var_iter_em is the #iterations until convergence of the M-step

DIGAMMA:
    simple: 23
    heavy: 9

LOG_SUM:
    simple: 4
    heavy: 2

LGAMMA: (if we use the function in utils.c)
    simple: 25
    heavy: 9

LDA_MLE
    simple: KV
    heavy: 2KV

RANDOM_INITIALIZE_SS
    simple: 2KN
    heavy: 2KN

LIKELIHOOD:
    simple: 6KD + 33K + 102
    heavy: 9K + 36

LDA_INFERENCE:
    simple: var_iter_inf * (38KD + 33K + 104) + 24K
    heavy: var_iter_inf * (14KD + 9K + 36) + KD + 10K

DOC_E_STEP:
    simple: var_iter_inf * (38KD + 33K + 104) + 4KN + 26K + 26
    heavy: var_iter_inf * (14KD + 9K + 36) + KD + 10K + 9

RUN_EM:
    simple: 2KN + KV + var_iter_em * (N * doc_e_step.simple + N + KV
              + 2 + N * lda_inference.simple))
    heavy: 2KN + 2KV + var_iter_em * (N * doc_step.heavy + 2KV
              + 1 + N * lda_inference.heavy)


