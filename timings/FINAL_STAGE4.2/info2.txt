Bench run 2017-05-27_13-27-23
Ran with the command-line `bench --n=10 --k=300,400,100 -fla`
Comment: "stage 4.2 xl k300 and k400"
===============================

OPTIONS
use_doubles=True
use_icc=True
use_long=True
END OPTIONS

Latest commit at time of writing:
690ee2eb (on branch Stage4.2-VecInline)

Additionally, the repo contained the following untracked files:
	Potential explanations.txt
	europarl_data.py
	meetings/GeorgMeeting2.txt
	notes/benchmark_exp_log.txt
	notes/skylake.png
	pltutils.py
	profiling/perfstats.txt
	timings/2017-05-15_20-09-52/fast_timings_50_100.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1000.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1100.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1200.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1300.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1400.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1500.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1600.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1700.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1800.csv
	timings/2017-05-15_20-09-52/fast_timings_50_1900.csv
	timings/2017-05-15_20-09-52/fast_timings_50_200.csv
	timings/2017-05-15_20-09-52/fast_timings_50_2000.csv
	timings/2017-05-15_20-09-52/fast_timings_50_300.csv
	timings/2017-05-15_20-09-52/fast_timings_50_400.csv
	timings/2017-05-15_20-09-52/fast_timings_50_500.csv
	timings/2017-05-15_20-09-52/fast_timings_50_600.csv
	timings/2017-05-15_20-09-52/fast_timings_50_700.csv
	timings/2017-05-15_20-09-52/fast_timings_50_800.csv
	timings/2017-05-15_20-09-52/fast_timings_50_900.csv
	timings/2017-05-15_20-09-52/info.txt
	timings/2017-05-16_16-07-00/info.txt
	timings/2017-05-16_16-07-00/slow_timings_50_100.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1000.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1100.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1200.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1300.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1400.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1500.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1600.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1700.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1800.csv
	timings/2017-05-16_16-07-00/slow_timings_50_1900.csv
	timings/2017-05-16_16-07-00/slow_timings_50_200.csv
	timings/2017-05-16_16-07-00/slow_timings_50_2000.csv
	timings/2017-05-16_16-07-00/slow_timings_50_300.csv
	timings/2017-05-16_16-07-00/slow_timings_50_400.csv
	timings/2017-05-16_16-07-00/slow_timings_50_500.csv
	timings/2017-05-16_16-07-00/slow_timings_50_600.csv
	timings/2017-05-16_16-07-00/slow_timings_50_700.csv
	timings/2017-05-16_16-07-00/slow_timings_50_800.csv
	timings/2017-05-16_16-07-00/slow_timings_50_900.csv
	timings/2017-05-17_21-35-26/info.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_100.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1000.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1100.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1200.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1300.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1400.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1500.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1600.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1700.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1800.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_1900.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_200.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_2000.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_300.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_400.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_500.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_600.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_700.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_800.txt
	timings/2017-05-17_21-35-26/perf_[slow]_50_900.txt
	timings/2017-05-17_21-42-58/info.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_100.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1000.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1100.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1200.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1300.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1400.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1500.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1600.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1700.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1800.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_1900.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_200.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_2000.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_300.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_400.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_500.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_600.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_700.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_800.txt
	timings/2017-05-17_21-42-58/perf_[slow]_50_900.txt
	timings/2017-05-17_21-44-36/info.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_100.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1000.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1100.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1200.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1300.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1400.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1500.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1600.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1700.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1800.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_1900.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_200.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_2000.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_300.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_400.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_500.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_600.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_700.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_800.txt
	timings/2017-05-17_21-44-36/perf_[fast]_50_900.txt
	timings/2017-05-17_21-45-02/info.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_100.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1000.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1100.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1200.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1300.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1400.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1500.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1600.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1700.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1800.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_1900.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_200.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_2000.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_300.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_400.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_500.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_600.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_700.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_800.txt
	timings/2017-05-17_21-45-02/perf_[fast]_50_900.txt
	timings/2017-05-18_20-20-17/fast_timings_20_200.csv
	timings/2017-05-18_20-20-17/info.txt
	timings/2017-05-18_20-22-20/fast_timings_20_200.csv
	timings/2017-05-18_20-22-20/info.txt
	timings/2017-05-19_18-29-18/fast_timings_50_1000.csv
	timings/2017-05-19_18-29-18/info.txt
	timings/2017-05-19_18-29-18/perf_[fast]_50_1000.txt
	timings/2017-05-19_18-42-35/fast_timings_50_2246.csv
	timings/2017-05-19_18-42-35/info.txt
	timings/2017-05-19_18-42-35/perf_fast_50_2246.txt
	timings/2017-05-19_18-42-37/fast_timings_50_1000.csv
	timings/2017-05-19_18-42-37/info.txt
	timings/2017-05-19_18-42-37/perf_[fast]_50_1000.txt
	timings/2017-05-19_21-09-28/fast_timings_50_1000.csv
	timings/2017-05-19_21-09-28/info.txt
	timings/2017-05-19_21-09-28/perf_fast_50_1000.txt
	timings/2017-05-23_13-10-58/fast_timings_50_1900.csv
	timings/2017-05-23_13-10-58/info.txt
	timings/2017-05-23_13-10-58/perf_[fast]_50_1900.txt
	timings/2017-05-26_00-05-44/fast_timings_400_10.csv
	timings/2017-05-26_00-05-44/info.txt
	timings/2017-05-26_00-05-44/perf_fast_400_10.txt
	timings/2017-05-26_00-23-19/info.txt
	timings/2017-05-26_00-23-19/perf_slow_100_10.txt
	timings/2017-05-26_00-23-19/slow_timings_100_10.csv
	timings/2017-05-26_15-20-04/info.txt
	timings/2017-05-26_15-20-04/perf_fast_28_500.txt
	timings/2017-05-27_12-07-29/info.txt
	timings/2017-05-27_12-07-29/perf_slow_10_100.txt
	timings/2017-05-27_12-07-29/slow_timings_10_100.csv
	timings/2017-05-27_13-27-23/info.txt
	timings/FINAL_BASELINE_NO-VEC/info.txt
	timings/FINAL_BASELINE_NO-VEC/perf_slow_100_10.txt
	timings/FINAL_BASELINE_NO-VEC/perf_slow_200_10.txt
	timings/FINAL_BASELINE_NO-VEC/perf_slow_300_10.txt
	timings/FINAL_BASELINE_NO-VEC/perf_slow_400_10.txt
	timings/FINAL_BASELINE_NO-VEC/slow_timings_100_10.csv
	timings/FINAL_BASELINE_NO-VEC/slow_timings_200_10.csv
	timings/FINAL_BASELINE_NO-VEC/slow_timings_300_10.csv
	timings/FINAL_BASELINE_NO-VEC/slow_timings_400_10.csv
	timings/FINAL_SLOW/info.txt
	timings/FINAL_SLOW/perf_slow_100_10.txt
	timings/FINAL_SLOW/perf_slow_200_10.txt
	timings/FINAL_SLOW/perf_slow_300_10.txt
	timings/FINAL_SLOW/perf_slow_400_10.txt
	timings/FINAL_SLOW/slow_timings_100_10.csv
	timings/FINAL_SLOW/slow_timings_200_10.csv
	timings/FINAL_SLOW/slow_timings_300_10.csv
	timings/FINAL_SLOW/slow_timings_400_10.csv
	timings/FINAL_STAGE1/fast_timings_100_10.csv
	timings/FINAL_STAGE1/fast_timings_200_10.csv
	timings/FINAL_STAGE1/fast_timings_300_10.csv
	timings/FINAL_STAGE1/fast_timings_400_10.csv
	timings/FINAL_STAGE1/info.txt
	timings/FINAL_STAGE1/perf_fast_100_10.txt
	timings/FINAL_STAGE1/perf_fast_200_10.txt
	timings/FINAL_STAGE1/perf_fast_300_10.txt
	timings/FINAL_STAGE1/perf_fast_400_10.txt
	timings/FINAL_STAGE1_NO-VEC/fast_timings_100_10.csv
	timings/FINAL_STAGE1_NO-VEC/fast_timings_200_10.csv
	timings/FINAL_STAGE1_NO-VEC/fast_timings_300_10.csv
	timings/FINAL_STAGE1_NO-VEC/fast_timings_400_10.csv
	timings/FINAL_STAGE1_NO-VEC/info.txt
	timings/FINAL_STAGE1_NO-VEC/perf_fast_100_10.txt
	timings/FINAL_STAGE1_NO-VEC/perf_fast_200_10.txt
	timings/FINAL_STAGE1_NO-VEC/perf_fast_300_10.txt
	timings/FINAL_STAGE1_NO-VEC/perf_fast_400_10.txt
	timings/FINAL_STAGE2/fast_timings_100_10.csv
	timings/FINAL_STAGE2/fast_timings_200_10.csv
	timings/FINAL_STAGE2/fast_timings_300_10.csv
	timings/FINAL_STAGE2/info.txt
	timings/FINAL_STAGE2/perf_fast_100_10.txt
	timings/FINAL_STAGE2/perf_fast_200_10.txt
	timings/FINAL_STAGE2/perf_fast_300_10.txt
	timings/FINAL_STAGE2_NO-VEC/.gitkeep
	timings/FINAL_STAGE2_NO-VEC/fast_timings_100_10.csv
	timings/FINAL_STAGE2_NO-VEC/fast_timings_200_10.csv
	timings/FINAL_STAGE2_NO-VEC/fast_timings_300_10.csv
	timings/FINAL_STAGE2_NO-VEC/fast_timings_400_10.csv
	timings/FINAL_STAGE2_NO-VEC/info.txt
	timings/FINAL_STAGE2_NO-VEC/perf_fast_100_10.txt
	timings/FINAL_STAGE2_NO-VEC/perf_fast_200_10.txt
	timings/FINAL_STAGE2_NO-VEC/perf_fast_300_10.txt
	timings/FINAL_STAGE2_NO-VEC/perf_fast_400_10.txt
	timings/FINAL_STAGE3.2/fast_timings_100_10.csv
	timings/FINAL_STAGE3.2/fast_timings_200_10.csv
	timings/FINAL_STAGE3.2/fast_timings_300_10.csv
	timings/FINAL_STAGE3.2/fast_timings_400_10.csv
	timings/FINAL_STAGE3.2/info.txt
	timings/FINAL_STAGE3.2/perf_fast_100_10.txt
	timings/FINAL_STAGE3.2/perf_fast_200_10.txt
	timings/FINAL_STAGE3.2/perf_fast_300_10.txt
	timings/FINAL_STAGE3.2/perf_fast_400_10.txt
	timings/FINAL_STAGE3/fast_timings_100_10.csv
	timings/FINAL_STAGE3/fast_timings_200_10.csv
	timings/FINAL_STAGE3/fast_timings_300_10.csv
	timings/FINAL_STAGE3/fast_timings_400_10.csv
	timings/FINAL_STAGE3/info.txt
	timings/FINAL_STAGE3/perf_fast_100_10.txt
	timings/FINAL_STAGE3/perf_fast_200_10.txt
	timings/FINAL_STAGE3/perf_fast_300_10.txt
	timings/FINAL_STAGE3/perf_fast_400_10.txt
	timings/FINAL_STAGE4.2/fast_timings_100_10.csv
	timings/FINAL_STAGE4.2/fast_timings_200_10.csv
	timings/FINAL_STAGE4.2/info.txt
	timings/FINAL_STAGE4.2/perf_fast_100_10.txt
	timings/FINAL_STAGE4.2/perf_fast_200_10.txt
	timings/FINAL_STAGE4/fast_timings_100_10.csv
	timings/FINAL_STAGE4/fast_timings_200_10.csv
	timings/FINAL_STAGE4/fast_timings_300_10.csv
	timings/FINAL_STAGE4/fast_timings_400_10.csv
	timings/FINAL_STAGE4/info.txt
	timings/FINAL_STAGE4/perf_fast_100_10.txt
	timings/FINAL_STAGE4/perf_fast_200_10.txt
	timings/FINAL_STAGE4/perf_fast_300_10.txt
	timings/FINAL_STAGE4/perf_fast_400_10.txt
	timings/FINAL_STAGE5/fast_timings_100_10.csv
	timings/FINAL_STAGE5/fast_timings_200_10.csv
	timings/FINAL_STAGE5/fast_timings_300_10.csv
	timings/FINAL_STAGE5/fast_timings_400_10.csv
	timings/FINAL_STAGE5/info.txt
	timings/FINAL_STAGE5/perf_fast_100_10.txt
	timings/FINAL_STAGE5/perf_fast_200_10.txt
	timings/FINAL_STAGE5/perf_fast_300_10.txt
	timings/FINAL_STAGE5/perf_fast_400_10.txt
	timings/fast-and-slow-k-200-n-all-original-dataset/fast_timings_200_2246.csv
	timings/fast-and-slow-k-200-n-all-original-dataset/info.txt
	timings/fast-and-slow-k-200-n-all-original-dataset/perf_fast_200_2246.txt
	timings/fast-and-slow-k-200-n-all-original-dataset/perf_slow_200_2246.txt
	timings/fast-and-slow-k-200-n-all-original-dataset/slow_timings_200_2246.csv
	timings/fast-meeting/fast_timings_50_100.csv
	timings/fast-meeting/fast_timings_50_1100.csv
	timings/fast-meeting/fast_timings_50_1300.csv
	timings/fast-meeting/fast_timings_50_1500.csv
	timings/fast-meeting/fast_timings_50_1700.csv
	timings/fast-meeting/fast_timings_50_1900.csv
	timings/fast-meeting/fast_timings_50_300.csv
	timings/fast-meeting/fast_timings_50_500.csv
	timings/fast-meeting/fast_timings_50_700.csv
	timings/fast-meeting/fast_timings_50_900.csv
	timings/fast-meeting/info.txt
	timings/fast-meeting/perf_fast_50_100.txt
	timings/fast-meeting/perf_fast_50_1100.txt
	timings/fast-meeting/perf_fast_50_1300.txt
	timings/fast-meeting/perf_fast_50_1500.txt
	timings/fast-meeting/perf_fast_50_1700.txt
	timings/fast-meeting/perf_fast_50_1900.txt
	timings/fast-meeting/perf_fast_50_300.txt
	timings/fast-meeting/perf_fast_50_500.txt
	timings/fast-meeting/perf_fast_50_700.txt
	timings/fast-meeting/perf_fast_50_900.txt
	timings/fast_europarl-10-docs/fast_timings_100_10.csv
	timings/fast_europarl-10-docs/fast_timings_120_10.csv
	timings/fast_europarl-10-docs/fast_timings_140_10.csv
	timings/fast_europarl-10-docs/fast_timings_160_10.csv
	timings/fast_europarl-10-docs/fast_timings_180_10.csv
	timings/fast_europarl-10-docs/fast_timings_200_10.csv
	timings/fast_europarl-10-docs/fast_timings_60_10.csv
	timings/fast_europarl-10-docs/fast_timings_80_10.csv
	timings/fast_europarl-10-docs/info.txt
	timings/fast_europarl-10-docs/perf_fast_100_10.txt
	timings/fast_europarl-10-docs/perf_fast_120_10.txt
	timings/fast_europarl-10-docs/perf_fast_140_10.txt
	timings/fast_europarl-10-docs/perf_fast_160_10.txt
	timings/fast_europarl-10-docs/perf_fast_180_10.txt
	timings/fast_europarl-10-docs/perf_fast_200_10.txt
	timings/fast_europarl-10-docs/perf_fast_60_10.txt
	timings/fast_europarl-10-docs/perf_fast_80_10.txt
	timings/slow-roofline-II/info.txt
	timings/slow-roofline-II/perf_slow_50_100.txt
	timings/slow-roofline-II/perf_slow_50_1100.txt
	timings/slow-roofline-II/perf_slow_50_1300.txt
	timings/slow-roofline-II/perf_slow_50_1500.txt
	timings/slow-roofline-II/perf_slow_50_1700.txt
	timings/slow-roofline-II/perf_slow_50_1900.txt
	timings/slow-roofline-II/perf_slow_50_300.txt
	timings/slow-roofline-II/perf_slow_50_500.txt
	timings/slow-roofline-II/perf_slow_50_700.txt
	timings/slow-roofline-II/perf_slow_50_900.txt
	timings/slow-roofline-II/slow_timings_50_100.csv
	timings/slow-roofline-II/slow_timings_50_1100.csv
	timings/slow-roofline-II/slow_timings_50_1300.csv
	timings/slow-roofline-II/slow_timings_50_1500.csv
	timings/slow-roofline-II/slow_timings_50_1700.csv
	timings/slow-roofline-II/slow_timings_50_1900.csv
	timings/slow-roofline-II/slow_timings_50_300.csv
	timings/slow-roofline-II/slow_timings_50_500.csv
	timings/slow-roofline-II/slow_timings_50_700.csv
	timings/slow-roofline-II/slow_timings_50_900.csv
	timings/slow-roofline/info.txt
	timings/slow-roofline/perf_slow_50_100.txt
	timings/slow-roofline/perf_slow_50_1100.txt
	timings/slow-roofline/perf_slow_50_1300.txt
	timings/slow-roofline/perf_slow_50_1500.txt
	timings/slow-roofline/perf_slow_50_1700.txt
	timings/slow-roofline/perf_slow_50_1900.txt
	timings/slow-roofline/perf_slow_50_300.txt
	timings/slow-roofline/perf_slow_50_500.txt
	timings/slow-roofline/perf_slow_50_700.txt
	timings/slow-roofline/perf_slow_50_900.txt
	timings/slow-roofline/slow_timings_50_100.csv
	timings/slow-roofline/slow_timings_50_1100.csv
	timings/slow-roofline/slow_timings_50_1300.csv
	timings/slow-roofline/slow_timings_50_1500.csv
	timings/slow-roofline/slow_timings_50_1700.csv
	timings/slow-roofline/slow_timings_50_1900.csv
	timings/slow-roofline/slow_timings_50_300.csv
	timings/slow-roofline/slow_timings_50_500.csv
	timings/slow-roofline/slow_timings_50_700.csv
	timings/slow-roofline/slow_timings_50_900.csv
	timings/slow_europarl-10-docs/info.txt
	timings/slow_europarl-10-docs/perf_slow_100_10.txt
	timings/slow_europarl-10-docs/perf_slow_120_10.txt
	timings/slow_europarl-10-docs/perf_slow_140_10.txt
	timings/slow_europarl-10-docs/perf_slow_160_10.txt
	timings/slow_europarl-10-docs/perf_slow_180_10.txt
	timings/slow_europarl-10-docs/perf_slow_200_10.txt
	timings/slow_europarl-10-docs/perf_slow_60_10.txt
	timings/slow_europarl-10-docs/perf_slow_80_10.txt
	timings/slow_europarl-10-docs/slow_timings_100_10.csv
	timings/slow_europarl-10-docs/slow_timings_120_10.csv
	timings/slow_europarl-10-docs/slow_timings_140_10.csv
	timings/slow_europarl-10-docs/slow_timings_160_10.csv
	timings/slow_europarl-10-docs/slow_timings_180_10.csv
	timings/slow_europarl-10-docs/slow_timings_200_10.csv
	timings/slow_europarl-10-docs/slow_timings_60_10.csv
	timings/slow_europarl-10-docs/slow_timings_80_10.csv
	timings/stage2/fast_timings_50_100.csv
	timings/stage2/fast_timings_50_1000.csv
	timings/stage2/fast_timings_50_1100.csv
	timings/stage2/fast_timings_50_1200.csv
	timings/stage2/fast_timings_50_1300.csv
	timings/stage2/fast_timings_50_1400.csv
	timings/stage2/fast_timings_50_1500.csv
	timings/stage2/fast_timings_50_1600.csv
	timings/stage2/fast_timings_50_1700.csv
	timings/stage2/fast_timings_50_1800.csv
	timings/stage2/fast_timings_50_1900.csv
	timings/stage2/fast_timings_50_200.csv
	timings/stage2/fast_timings_50_2000.csv
	timings/stage2/fast_timings_50_300.csv
	timings/stage2/fast_timings_50_400.csv
	timings/stage2/fast_timings_50_500.csv
	timings/stage2/fast_timings_50_600.csv
	timings/stage2/fast_timings_50_700.csv
	timings/stage2/fast_timings_50_800.csv
	timings/stage2/fast_timings_50_900.csv
	timings/stage2/info.txt
	timings/stage2/perf_[fast]_50_100.txt
	timings/stage2/perf_[fast]_50_1000.txt
	timings/stage2/perf_[fast]_50_1100.txt
	timings/stage2/perf_[fast]_50_1200.txt
	timings/stage2/perf_[fast]_50_1300.txt
	timings/stage2/perf_[fast]_50_1400.txt
	timings/stage2/perf_[fast]_50_1500.txt
	timings/stage2/perf_[fast]_50_1600.txt
	timings/stage2/perf_[fast]_50_1700.txt
	timings/stage2/perf_[fast]_50_1800.txt
	timings/stage2/perf_[fast]_50_1900.txt
	timings/stage2/perf_[fast]_50_200.txt
	timings/stage2/perf_[fast]_50_2000.txt
	timings/stage2/perf_[fast]_50_300.txt
	timings/stage2/perf_[fast]_50_400.txt
	timings/stage2/perf_[fast]_50_500.txt
	timings/stage2/perf_[fast]_50_600.txt
	timings/stage2/perf_[fast]_50_700.txt
	timings/stage2/perf_[fast]_50_800.txt
	timings/stage2/perf_[fast]_50_900.txt

The diff, if any, follows:

diff --git a/.gitignore b/.gitignore
index 2e87c36..d546036 100644
--- a/.gitignore
+++ b/.gitignore
@@ -18,7 +18,11 @@ obj/*
 # Compile output
 **/*.o
 **/lda
-**/lda.exe
+
+# Windows
+**/*.exe
+**/*.ilk
+**/*.pdb
 
 # Output files from running
 **/*.gamma
diff --git a/VECTORIZATION_HOWTO.txt b/VECTORIZATION_HOWTO.txt
index 448f707..c4416db 100644
--- a/VECTORIZATION_HOWTO.txt
+++ b/VECTORIZATION_HOWTO.txt
@@ -30,7 +30,7 @@ differentiate between floats and doubles. Therefore, you do things like:
     __m256fp ab = _mm256_add(a, b)
     __m256fp ab_sq = _mm256_mul(ab, ab)
 
-Once again, the correct version is selected by the DOUBLE define. Note that not
+Once again, the correct version is selected by the FLOAT define. Note that not
 all intrinsics are defined in this way. Specifically, I did not define the ones
 that have markedly different behavior between floats and doubles (e.g. unpacks).
 If you need to use such an intrinsic, talk to me.
diff --git a/benchmarking.py b/benchmarking.py
index 5592a14..e65fb58 100755
--- a/benchmarking.py
+++ b/benchmarking.py
@@ -1,191 +1,17 @@
-#! /usr/bin/python
 import numpy as np
-import sys
 import matplotlib.pyplot as plt
+import sys
 from os import listdir
 from os.path import join
 from os.path import dirname
-import re
-
-EPS = 1e-12     # Average count is float and integer count is a text in case of 0 calls
-
-D = None
-V = None
-
-#def memoize(f):
-#    cache = {}
-#    return lambda *args: cache[args] if args in cache else cache.update({args: f(*args)}) or cache[args]
-
-class Cost:
-    def __init__(self, adds=0, muls=0, divs=0, logs=0, exps=0):
-        if isinstance(adds, tuple):
-            self.adds = adds[0]
-            self.muls = adds[1]
-            self.divs = adds[2]
-            self.logs = adds[3]
-            self.exps = adds[4]
-        else:
-            self.adds = adds
-            self.muls = muls
-            self.divs = divs
-            self.logs = logs
-            self.exps = exps
-
-    def toTup(self):
-        return (self.adds, self.muls, self.divs, self.logs, self.exps)
-
-    def __str__(self):
-        return str(self.toTup())
-
-    def __add__(self, other):
-        if isinstance(other, tuple):
-            other = Cost(other)
-        return Cost(self.adds + other.adds, \
-                    self.muls + other.muls, \
-                    self.divs + other.divs, \
-                    self.logs + other.logs, \
-                    self.exps + other.exps)
-
-    def __mul__(self, other):
-        if isinstance(other, int) or isinstance(other, float):
-            return Cost(self.adds * other, \
-                        self.muls * other, \
-                        self.divs * other, \
-                        self.logs * other, \
-                        self.exps * other)
-        else:
-            return NotImplemented     # multiplication not defined
-
-    def __rmul__(self, other):
-        return self.__mul__(other)
-
-    def __radd__(self, other):
-        return self.__add__(other)
-
-    def full(self):
-        return  self.adds + \
-                self.muls + \
-                self.divs + \
-                self.logs + \
-                self.exps
-
-iters = {"EM_CONVERGE" : 1, "INFERENCE_CONVERGE" : 1., "ALPHA_CONVERGE" : 1.}  # conv counts for var iterations
-
-#@memoize
-def digamma(N, K):
-    return Cost(18, 5, 8, 1, 0)
-
-#@memoize
-def log_sum(N, K):
-    return Cost(4, 0, 0, 1, 1)
-
-#@memoize
-def log_gamma(N, K):
-    return Cost(20, 5, 2, 7, 0)
-
-#@memoize
-def trigamma(N, K):
-    return Cost(7, 7, 2, 0, 0) + 6 * Cost(2, 1, 1, 0, 0)
-
-#@memoize
-def random_initialize_ss(N, K):
-    return K * N * Cost(adds=3, divs=1)
-
-#@memoize
-def opt_alpha(N, K):
-    return iters["ALPHA_CONVERGE"] * ( \
-            Cost(adds=2, muls=1, divs=1, exps=1) + Cost(adds=3, muls=4) + 2 * log_gamma(N, K) + \
-            Cost(adds=2, muls=4) + 2 * digamma(N, K) + Cost(adds=1, muls=5) + 2 * trigamma(N, K)) + \
-            Cost(exps=1) # poor lonely exp
-
-#@memoize
-def mle(N, K):
-    return N * K * Cost(adds=2, logs=2) + opt_alpha(N, K)
-
-#@memoize
-def likelihood(N, K):
-    return K * digamma(N, K) + Cost(adds=K) + digamma(N, K) + Cost(adds=2, muls=2) + 3 * log_gamma(N, K) + \
-            K * Cost(adds=7, muls=2) + K * D * 6 * Cost(adds=4, muls=2, logs=1)
-
-#@memoize
-def lda_inference(N, K):
-    return K * Cost(adds=1, divs=1) + K * digamma(N, K) + K * D * Cost(divs=1) + \
-        iters["INFERENCE_CONVERGE"] * (D * K * (digamma(N, K) + Cost(adds=4, muls=1, exps=1) + log_sum(N, K)) + \
-            likelihood(N, K) + Cost(adds=1, divs=1))
-
-#@memoize
-def doc_e_step(N, K):
-    return lda_inference(N, K) + K * Cost(adds=2) + Cost(adds=1, muls=1) + digamma(N, K) + K * N * Cost(adds=2, muls=2)
-
-#@memoize
-def run_em(N, K):
-    return random_initialize_ss(N, K) + mle(N, K) + iters["EM_CONVERGE"] * (N * doc_e_step(N, K) + Cost(adds=N) + \
-            mle(N, K) + Cost(adds=1, muls=1, divs=1))
-
-flops = { "RUN_EM" : run_em, "LDA_INFERENCE" : lda_inference, "DIGAMMA" : digamma, "LOG_SUM" : log_sum,
-        "LOG_GAMMA" : log_gamma, "TRIGAMMA" : trigamma, "DOC_E_STEP" : doc_e_step, "LIKELIHOOD" : likelihood, "MLE" : mle, "OPT_ALPHA" : opt_alpha}
+import pltutils
 
 colors = { "RUN_EM" : "green", "LDA_INFERENCE" : "blue", "DIGAMMA" : "black", "LOG_SUM" : "purple",
-        "LOG_GAMMA" : "purple", "TRIGAMMA" : "cyan", "DOC_E_STEP" : "orange", "LIKELIHOOD" : "red", "MLE" : "cyan", "OPT_ALPHA" : "cyan"}
+        "LOG_GAMMA" : "purple", "TRIGAMMA" : "cyan", "DOC_E_STEP" : "orange", "LIKELIHOOD" : "red", "MLE" : "#5E6382", "OPT_ALPHA" : "#00bcd4"}
 
 label_offsets = { "RUN_EM" : 0.008, "LDA_INFERENCE" : 0.008, "DIGAMMA" : 0.02, "LOG_SUM" : -0.015,
         "LOG_GAMMA" : 0.008, "TRIGAMMA" : 0.01, "DOC_E_STEP" : -0.01, "LIKELIHOOD" : -0.015, "MLE" : 0.01, "OPT_ALPHA" : 0.01 }
 
-def set_corpus_stats(location):
-    global D
-    global V
-    with open(location + '/info.txt') as f:
-        found = False
-        ln = 'This is the line'
-        while ln != '':
-            ln = f.readline()
-            if ln.startswith('use_long'):
-                found = True
-                # A little bird told us that these were the proper values
-                if ln.endswith('True\n'): # Long corpus
-                    D = 11002
-                    V = 48613
-                else: # Regular corpus
-                    D = 135
-                    V = 10473
-                break
-
-        if not found:
-            print('Warning: use_long setting not found, assuming ap corpus...')
-            D = 135
-            V = 10473
-
-def read_one_output(f, N, K, dic):
-    header = f.readline().split(',')
-    header = [h.strip() for h in header]
-    assert header[0] == 'Accumulator' \
-        and header[1] == 'Total count' \
-       and header[2] == 'Average count', "Output file not in proper format"
-    lines = f.readlines()
-
-
-    # Naming convention: All convergence counters should include the tag 'CONVERGE'
-    for i in range(len(lines)):
-        s = lines[i].split(',')
-        fn = s[0].strip()
-        if 'CONVERGE' in fn:
-            iters[fn] = float(s[2])
-
-    for i in range(len(lines)):
-        s = lines[i].split(',')
-        fn = s[0].strip()
-        if 'CONVERGE' not in fn:
-            if not fn in dic: dic[fn] = { 'x' : [], 'y' : []}
-            avg_cnt = float(s[2])
-            if abs(avg_cnt) > EPS:
-                dic[fn]['x'].append(N)
-                dic[fn]['y'].append(flops[fn](N, K).full() / avg_cnt)
-            pass
-        pass
-    pass
-
-    print(iters)
-
 def plot_line(plt, data, color=None, label_offset=None):
     for fn, vals in data.items():
         if len(vals['x']) is not 0:
@@ -198,15 +24,15 @@ def plot_line(plt, data, color=None, label_offset=None):
                 use_label_offset = label_offsets[fn]
             else:
                 use_label_offset = label_offset
-            plt.plot(x, y, '^-', color=use_color, linewidth=1)
+            plt.plot(x, y, '^-', color=use_color, linewidth=2)
             plt.text(x[0], y[0] + use_label_offset , fn, color=use_color, size=9)
 
-def set_up_perf_plot(axes):
+def set_up_perf_plot(axes, xaxis = 'N'):
     #Per plot settings
     axes.set_title('Performance on Skylake',  y=1.08, loc = "center")
-    axes.set_xlabel('$n$ documents')
+    axes.set_xlabel('$k$ topics' if xaxis == 'K' else '$n$ documents')
     axes.set_ylabel('Performance [flops/cycles]',rotation="0")
-    axes.set_ylim(-0.0, 0.4)
+    #axes.set_ylim(-0.0, 0.4)
 
     axes.set_axisbelow(True)
     axes.yaxis.grid(color='white', linestyle='solid')
@@ -220,30 +46,42 @@ def set_up_perf_plot(axes):
     #plt.axhline(y=4, linestyle='--', color='black', label='Compute roof')
 
 
-def benchmark(dirpath):
+def benchmark(dirpath, vec=False, xaxis='N'):
+    pltutils.set_corpus_stats(dirpath)
     data_1 = {}       # "RUN_EM" : { x : [10, 15, 20], y : [3.2, 4.5, 6.7] }
     data_2 = {}       # "RUN_EM" : { x : [10, 15, 20], y : [3.2, 4.5, 6.7] }
 
     _, axes = plt.subplots()
 
-
-    regex = re.compile(r'\d+')
     for filename in listdir(dirpath):
         if filename.startswith("fast") or filename.startswith("slow"):
-            f = open(join(dirpath, filename), "r")
-            K, N = map(int, re.findall(regex, filename))
-            if filename[0] == 'f':
-                read_one_output(f, N, K, data_1)
-            else:
-                read_one_output(f, N, K, data_2)
+            K, N, _, _, perf = pltutils.read_one_output(join(dirpath, filename), vec=vec)
+            dic = data_1 if filename[0] == 'f' else data_2
+            for i, fn in enumerate(pltutils.fns):
+                if not fn in dic: dic[fn] = {'x' : [], 'y' : []}
+                dic[fn]['x'].append(K if xaxis == 'K' else N)  
+                dic[fn]['y'].append(perf[i])  
 
-    set_up_perf_plot(axes)
+    set_up_perf_plot(axes, xaxis)
 
     plot_line(plt, data_1)
     plot_line(plt, data_2)
     plt.show()
 
+def usage_and_quit():
+    print("\nPerformance of all timers for one batch of runs")
+    print("\npython benchmarking.py <dirname> x axis = <N|K> ?vec")
+    sys.exit(1)
+
 if __name__ == '__main__':
-    benchmark(sys.argv[1])
+    if str(sys.argv[1]) in {"-h", "--help"}:
+        usage_and_quit()
+    if len(sys.argv) < 3:
+        print('Wrong number of arguments')
+        usage_and_quit()
+    vec = False
+    if sys.argv[-1] == 'vec':
+        vec = True
+    benchmark(sys.argv[1], vec, sys.argv[2])
 
 
diff --git a/chart.py b/chart.py
index fe28261..ffee006 100644
--- a/chart.py
+++ b/chart.py
@@ -1,156 +1,27 @@
-#! /usr/bin/python
 import numpy as np
-import sys
 import matplotlib.pyplot as plt
-import os.path
-import re
-import benchmarking
-
-PURIFY = False
-
-D = None
-V = None
-EPS = 1e-8
-
-iters = {"EM_CONVERGE" : 0, "INFERENCE_CONVERGE" : 0, "ALPHA_CONVERGE" : 0}  # conv counts for var iterations
-
-fns = ["RUN_EM", "LDA_INFERENCE", "DIGAMMA", "LOG_SUM", "DOC_E_STEP", "LIKELIHOOD", "MLE", "OPT_ALPHA", "TRIGAMMA", "LOG_GAMMA"]
-
-def digamma(N, K):
-    return 0
-
-def log_sum(N, K):
-    return 0
-
-def log_gamma(N, K):
-    return 0
-
-def trigamma(N, K):
-    return 0 + 6 * 0
-
-def random_initialize_ss(N, K):
-    return K * N * 0
-
-def opt_alpha(N, K):
-    return iters["ALPHA_CONVERGE"] * ( \
-            0 + 0 + 2 * avg_cycles["LOG_GAMMA"] + \
-            0 + 2 * avg_cycles["DIGAMMA"] + 0 + 2 * avg_cycles["TRIGAMMA"]) + \
-            0 # poor lonely exp
-
-def mle(N, K):
-    return N * K * 0 + avg_cycles["OPT_ALPHA"]
-
-def likelihood(N, K):
-    return K * avg_cycles["DIGAMMA"] + 0 + avg_cycles["DIGAMMA"] + 0 + 3 * avg_cycles["LOG_GAMMA"] + \
-            K * 0 + K * D * 6 * 0
-
-def lda_inference(N, K):
-    return K * 0 + K * avg_cycles["DIGAMMA"] + K * D * 0 + \
-        iters["INFERENCE_CONVERGE"] * (D * K * (avg_cycles["DIGAMMA"] + 0 + avg_cycles["LOG_SUM"]) + \
-            avg_cycles["LIKELIHOOD"] + 0)
-
-def doc_e_step(N, K):
-    return avg_cycles["LDA_INFERENCE"] + K * 0 + 0 + avg_cycles["DIGAMMA"] + K * N * 0
-
-def run_em(N, K):
-    return 0 + avg_cycles["MLE"] + iters["EM_CONVERGE"] * (N * avg_cycles["DOC_E_STEP"] + 0 + \
-            avg_cycles["MLE"] + 0)
-
-impurity = { "RUN_EM" : run_em, "LDA_INFERENCE" : lda_inference, "DIGAMMA" : digamma, "LOG_SUM" : log_sum,
-        "LOG_GAMMA" : log_gamma, "TRIGAMMA" : trigamma, "DOC_E_STEP" : doc_e_step, "LIKELIHOOD" : likelihood, "MLE" : mle, "OPT_ALPHA" : opt_alpha}
-
-tot_cycles = {"RUN_EM" : 0., "LDA_INFERENCE" : 0., "DIGAMMA" : 0., "LOG_SUM" : 0., "DOC_E_STEP" : 0., "LIKELIHOOD" : 0., "MLE" : 0., \
-        "OPT_ALPHA" : 0., "TRIGAMMA" : 0., "LOG_GAMMA" : 0.}
-pur_cycles = {"RUN_EM" : 0., "LDA_INFERENCE" : 0., "DIGAMMA" : 0., "LOG_SUM" : 0., "DOC_E_STEP" : 0., "LIKELIHOOD" : 0., "MLE" : 0., \
-        "OPT_ALPHA" : 0., "TRIGAMMA" : 0., "LOG_GAMMA" : 0.}
-avg_cycles = {"RUN_EM" : 0., "LDA_INFERENCE" : 0., "DIGAMMA" : 0., "LOG_SUM" : 0., "DOC_E_STEP" : 0., "LIKELIHOOD" : 0., "MLE" : 0., \
-        "OPT_ALPHA" : 0., "TRIGAMMA" : 0., "LOG_GAMMA" : 0.}
-
-def set_corpus_stats(location):
-    global D
-    global V
-
-    with open(location + '/info.txt') as f:
-        found = False
-        ln = 'This is the line'
-        while ln != '':
-            ln = f.readline()
-            if ln.startswith('use_long'):
-                found = True
-                # A little bird told us that these were the proper values
-                if ln.endswith('True\n'): # Long corpus
-                    D = 11002
-                    V = 48613
-                else: # Regular corpus
-                    D = 135
-                    V = 10473
-                break
-
-        if not found:
-            print('Warning: use_long setting not found, assuming ap corpus...')
-            D = 135
-            V = 10473
-
-
-def read_one_output(fname, K, N):
-    if D is None:
-        set_corpus_stats(os.path.dirname(fname) or '.')
-        benchmarking.set_corpus_stats(os.path.dirname(fname) or '.')
-
-    f = open(fname, "r")
-    header = f.readline().split(',')
-    header = [h.strip() for h in header]
-    assert header[0] == 'Accumulator' \
-        and header[1] == 'Total count' \
-       and header[2] == 'Average count', "Output file not in proper format"
-    lines = f.readlines()
-
-    for fn in fns:
-        tot_cycles[fn] = 0.
-        pur_cycles[fn] = 0.
-        avg_cycles[fn] = 0.
-        pass
-    for itr in iters: iters[itr] = 0.
-
-    for i in range(len(lines)):
-        s = lines[i].split(',')
-        fn = s[0].strip()
-        if 'CONVERGE' not in fn:
-            if s[2].strip() is not '0':     # code inside function not called case
-                tot_cnt = float(s[1])
-                avg_cnt = float(s[2])
-                tot_cycles[fn] = tot_cnt
-                avg_cycles[fn] = avg_cnt
-        else:
-            iters[fn] = float(s[2])
-        pass
-
-    ret_flop_list = [0] * len(fns)
-    if PURIFY:
-        # purifying
-        for i, fn in enumerate(fns):
-            pur_cycles[fn] = tot_cycles[fn] - impurity[fn](N, K)
-            print("cycles ", fn, tot_cycles[fn], pur_cycles[fn])
-            ret_flop_list[i] = pur_cycles[fn]
-    else:
-        for i, fn in enumerate(fns):
-            ret_flop_list[i] = tot_cycles[fn]
-
-    return ret_flop_list
+import sys
+from os import listdir
+from os.path import join
+from os.path import dirname
+import pltutils
+from pltutils import fns
+import os
 
 colors = { "RUN_EM" : "green", "LDA_INFERENCE" : "blue", "DIGAMMA" : "black", "LOG_SUM" : "purple",
-        "LOG_GAMMA" : "purple", "TRIGAMMA" : "cyan", "DOC_E_STEP" : "orange", "LIKELIHOOD" : "red", "MLE" : "cyan", "OPT_ALPHA" : "cyan"}
+        "LOG_GAMMA" : "purple", "TRIGAMMA" : "cyan", "DOC_E_STEP" : "orange", "LIKELIHOOD" : "red", "MLE" : "#5E6382", "OPT_ALPHA" : "#00bcd4"}
 
-def stacked_bar_plot(filenames, KNs, xticklabels=None):
+def stacked_bar_plot(filenames, xticklabels=None):
     cycles = []
-    for i, f in enumerate(filenames):
-        cls = read_one_output(f, KNs[i][0], KNs[i][1])
+    for i, filename in enumerate(filenames):
+        pltutils.set_corpus_stats(dirname(filename))
+        _, _, _, cls, _ = pltutils.read_one_output(filename)
         cycles.append(cls)
         pass
     cycles = np.array(cycles)
     cycles = np.transpose(cycles)
 
-    width = 1
+    width = 0.75
     ind = np.arange(len(filenames))
     bottom = np.zeros(len(ind))
     fig, ax = plt.subplots()
@@ -163,24 +34,25 @@ def stacked_bar_plot(filenames, KNs, xticklabels=None):
     if xticklabels is not None:
         ax.set_xticklabels(xticklabels)
 
-    ax.set_ylabel('flop count')
-    ax.set_title('Flop counts for different runs per group')
+    ax.set_ylabel('cycle component')
+    ax.set_title('#cycle counts for different runs per group')
     ax.set_xticks(ind)
     plt.show()
 
 
-colors2 = ['red', 'blue', 'green', 'yellow', 'cyan']
-def bar_plot(filenames, KNs, legends=None):
+colors2 = ['red', 'blue', 'orange', 'yellow', 'cyan']
+def bar_plot(filenames, legends=None):
     width = 1 / (len(filenames) + 1.)   # width of bars
     ind = np.arange(len(fns))
     fig, ax = plt.subplots()
     p = [None] * len(filenames)
-    for i in range(len(filenames)):
-        cycles = read_one_output(filenames[i], KNs[i][0], KNs[i][1])
+    for i, filename in enumerate(filenames):
+        pltutils.set_corpus_stats(dirname(filename))
+        _, _, _, cycles, _ = pltutils.read_one_output(filename)
         p[i] = ax.bar(ind + i * width, cycles, width, color = colors2[i])
 
-    ax.set_ylabel('flop count')
-    ax.set_title('Flop counts for different runs per group')
+    ax.set_ylabel('cycle count')
+    ax.set_title('Cycles counts for different runs per group')
     ax.set_xticks(ind)
     ax.set_xticklabels(fns)
     ax.set_yscale("log")
@@ -189,19 +61,18 @@ def bar_plot(filenames, KNs, legends=None):
     ax.grid(linestyle='--', linewidth=2, axis='y')
     plt.show()
 
-def perf_plot(filenames, KNs, legends=None):
+def perf_plot(filenames, legends=None):
     width = 1 / (len(filenames) + 1.)   # width of bars
     ind = np.arange(len(fns))
     fig, ax = plt.subplots()
     p = [None] * len(filenames)
+    #print(fns)
     for i, filename in enumerate(filenames):
-        read_one_output(filename, KNs[i][0], KNs[i][1])
-        benchmarking.iters = iters
-        perf = [0] * len(fns)
-        for j,fn in enumerate(fns):
-            if abs(avg_cycles[fn]) > EPS:
-                #print(fn, "Full flops", benchmarking.flops[fn](KNs[i][1], KNs[i][0]))
-                perf[j] = benchmarking.flops[fn](KNs[i][1], KNs[i][0]).full() / avg_cycles[fn]
+        pltutils.set_corpus_stats(dirname(filename))
+        _, _, flp, cls, perf = pltutils.read_one_output(filename)
+        #print(perf)
+        #print(flp)
+        #print(cls)
         p[i] = ax.bar(ind + i * width, perf, width, color = colors2[i])
     ax.set_ylabel('performance')
     ax.set_title('performance for different runs per group')
@@ -212,23 +83,24 @@ def perf_plot(filenames, KNs, legends=None):
     ax.grid(linestyle='--', linewidth=2, axis='y')
     plt.show()
 
+def usage_and_quit():
+    print("\ncompare any number of timing files for #cycles perf and #comp cycles")
+    print("\npython chart.py <filenames full path list...>")
+    print("\na good convention is to go from slow file to fast file")
+    sys.exit(1)
 
 if __name__ == "__main__":
-    if len(sys.argv) == 1: 
-        print("python chart.py <filenames list...>")
-        sys.exit(1)
+    if str(sys.argv[1]) in {"-h", "--help"} or len(sys.argv) == 1:
+        usage_and_quit()
     filenames = sys.argv[1:]
-    KNs = []
     legends = []
-    
-    regex = re.compile(r'\d+')
+
     for filename in filenames:
-        K, N = map(int, re.findall(regex, filename))
-        KNs.append((K, N))
-        pname = filename[:-1]
+
+        pname = os.path.basename(filename)
         something = pname.split('.')[0].split('_')
         legends.append(something[0] + " " + something[2] + " " + something[3])
 
-    bar_plot(filenames, KNs, legends)
-    perf_plot(filenames, KNs, legends)
-    stacked_bar_plot(filenames, KNs, legends)
+    bar_plot(filenames, legends)
+    perf_plot(filenames, legends)
+    stacked_bar_plot(filenames, legends)
diff --git a/compare_timings.py b/compare_timings.py
index 7109112..f122f57 100644
--- a/compare_timings.py
+++ b/compare_timings.py
@@ -6,32 +6,34 @@ from os import listdir
 from os.path import join
 
 import benchmarking
+import pltutils
+from pltutils import fns
 
-def read_one_timer_all_Ns(path, timer_name, label=""):
-    data = {}
-    regex = re.compile(r'\d+')
+def read_one_timer_all_Ns(path, timer_name, label="", xaxis='N'):
+    data = {'x' : [], 'y' : []}
+    pltutils.set_corpus_stats(path)
     for filename in listdir(path):
             if("timings" in filename):
-                f = open(join(path, filename), "r")
-                # Extract K and N from the filename
-                K, N = map(int, re.findall(regex, filename))
-                benchmarking.read_one_output(f, N, K, data)
+                fname = join(path, filename)
+                K, N, _, _, perf = pltutils.read_one_output(fname)
+                data['x'].append(K if xaxis == 'K' else N)
+                data['y'].append(perf[fns.index(timer_name)])
 
     # amend the label afterwards so we can obtain the automated cycle count for the timer
-    return {label + " " + timer_name : data[timer_name]}
+    return {label + " " + timer_name : data}
 
 
-def compare_timings(path1, path2, timer_name):
+def compare_timings(path1, path2, timer_name, xaxis='N'):
     """
     path1           directory with old timings       
     path2           directory with new timings
     timer_name      has to match the name of a timer in the timing files
     """
-    data_1 = read_one_timer_all_Ns(path1, timer_name, "old")    
-    data_2 = read_one_timer_all_Ns(path2, timer_name, "new")
+    data_1 = read_one_timer_all_Ns(path1, timer_name, "old", xaxis)    
+    data_2 = read_one_timer_all_Ns(path2, timer_name, "new", xaxis)
 
     _, axes = plt.subplots()
-    benchmarking.set_up_perf_plot(axes)
+    benchmarking.set_up_perf_plot(axes, xaxis)
 
     benchmarking.plot_line(plt, data_1, color="red", label_offset=0.01)
     benchmarking.plot_line(plt, data_2, color="green", label_offset=0.01)
@@ -39,15 +41,15 @@ def compare_timings(path1, path2, timer_name):
 
 def usage_and_quit():
     print("\nCompare performance of one function for two batches of timings")
-    print("\nUsage: python compare_timings.py old_path new_path2 timer_name")
+    print("\nUsage: python compare_timings.py old_path new_path2 timer_name N|K")
     print()
     sys.exit()
 
 if __name__ == '__main__':
     if str(sys.argv[1]) in {"-h", "--help"}:
         usage_and_quit()
-    if len(sys.argv) != 4:
+    if len(sys.argv) != 5:
         print('Wrong number of arguments')
         usage_and_quit()
 
-    compare_timings(str(sys.argv[1]), str(sys.argv[2]), str(sys.argv[3]))
\ No newline at end of file
+    compare_timings(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])
diff --git a/fast-lda/fp.c b/fast-lda/fp.c
index d6676e9..afe898b 100644
--- a/fast-lda/fp.c
+++ b/fast-lda/fp.c
@@ -1,16 +1,15 @@
 #include "fp.h"
 #include <math.h>
+#include <stdio.h>
 
-#ifdef DOUBLE
-    void printv(const char* s, __m256d x) {
-        double a[4];
-        _mm256_storeu_pd(a, x);
-        printf("%s [%f %f %f %f]\n", s, a[0], a[1], a[2], a[3]);
-    }
-#else
-    void printv(const char* s, __m256 x) {
-        float a[8];
-        _mm256_storeu_ps(a, x);
+#ifdef FLOAT
+	void printv(const char* s, __m256 x) {
+        float* a = (float*) &x;
         printf("%s [%f %f %f %f %f %f %f %f]\n", s, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7]);
     }
+#else
+   	void printv(const char* s, __m256d x) {
+        double* a = (double*) &x;
+        printf("%s [%f %f %f %f]\n", s, a[0], a[1], a[2], a[3]);
+    } 
 #endif
diff --git a/fast-lda/fp.h b/fast-lda/fp.h
index cc2c17a..16c7450 100644
--- a/fast-lda/fp.h
+++ b/fast-lda/fp.h
@@ -7,7 +7,7 @@ single and double precision. */
 #include <math.h>
 #include <immintrin.h>
 
-#define MAX(a, b) ((a) > (b) ? (a) : (b))
+#define MAX(a, b) ((a) > (b) ? (a) : (b)) 
 #ifdef FLOAT
     #define fp_t float
     #define ALIGNMENT             32
@@ -80,7 +80,9 @@ single and double precision. */
         #define _mm256_log          _mm256_log_ps
         #define _mm256_exp          _mm256_exp_ps
         #define _mm256_log1p        _mm256_log1p_ps
-    #endif // __INTEL_COMPILER
+    #endif
+
+
 
 #else
 
@@ -101,6 +103,7 @@ single and double precision. */
         }\
     }
 
+
     #define __m256fp            __m256d
 
     #define _mm256_add          _mm256_add_pd
@@ -143,13 +146,14 @@ single and double precision. */
         #define _mm256_log          _mm256_log_pd
         #define _mm256_exp          _mm256_exp_pd
         #define _mm256_log1p        _mm256_log1p_pd
-    #endif // __INTEL_COMPILER
+    #endif
 
     #define _mm256_rcp(a)       _mm256_div_pd(_mm256_set1_pd(1.0), a)
 
     // reciprocal * constant
     #define _rcp_const(c, a)    _mm256_div_pd(c, a)
 
-#endif // FLOAT
 
-#endif // VECTORIZE_H
\ No newline at end of file
+#endif
+
+#endif
diff --git a/fast-lda/lda-alpha.c b/fast-lda/lda-alpha.c
index 93684e9..abeb09c 100644
--- a/fast-lda/lda-alpha.c
+++ b/fast-lda/lda-alpha.c
@@ -44,7 +44,7 @@ fp_t d2_alhood(fp_t a, int N, int K)
 fp_t opt_alpha(fp_t ss, int N, int K)
 {
     fp_t a, log_a, init_a = 100;
-    fp_t f, df, d2f;
+    fp_t df, d2f;
     int iter = 0;
 
     timer t = start_timer(OPT_ALPHA);
@@ -61,11 +61,9 @@ fp_t opt_alpha(fp_t ss, int N, int K)
             a = init_a;
             log_a = log(a);
         }
-        f = alhood(a, ss, N, K);
         df = d_alhood(a, ss, N, K);
         d2f = d2_alhood(a, N, K);
         log_a = log_a - df/(d2f * a + df);
-        // printf("alpha maximization : %5.5f   %5.5f\n", f, df);
     }
     while ((fabs(df) > NEWTON_THRESH) && (iter < MAX_ALPHA_ITER));
 
diff --git a/fast-lda/lda-estimate-helper.c b/fast-lda/lda-estimate-helper.c
index a533fd5..12eecef 100644
--- a/fast-lda/lda-estimate-helper.c
+++ b/fast-lda/lda-estimate-helper.c
@@ -3,6 +3,7 @@
  */
 
 #include "lda-estimate-helper.h"
+#include "utils.h"
 
 void save_gamma(char* filename, fp_t** gamma, int num_docs, int num_topics)
 {
@@ -57,7 +58,7 @@ void gatherDocWords(fp_t* entire_matrix, fp_t* reduced_matrix, document* doc, si
 void scatterDocWords(fp_t* entire_matrix, fp_t* reduced_matrix, document* doc, size_t row_size) {
     for (int n = 0; n < doc->length; n++)
     {
-        for (int k = 0; k < row_size; k++)
+        for (unsigned int k = 0; k < row_size; k++)
             entire_matrix[doc->words[n] * row_size + k] = reduced_matrix[n * row_size + k];
     }
-}
+}
\ No newline at end of file
diff --git a/fast-lda/lda-estimate.c b/fast-lda/lda-estimate.c
index f6ecd50..ee8e743 100644
--- a/fast-lda/lda-estimate.c
+++ b/fast-lda/lda-estimate.c
@@ -58,7 +58,7 @@ fp_t doc_e_step(document* doc, fp_t* gamma, fp_t* phi,
     __m256fp alpha_accs = _mm256_setzero();
     for (k = 0; k < KK; k += STRIDE)
     {
-        __m256fp gams = _mm256_load(gamma + k);
+        __m256fp gams = _mm256_loadu(gamma + k);
         //gamma_sum += gamma[k];
         gamma_accs = _mm256_add(gamma_accs, gams);
 
@@ -68,7 +68,9 @@ fp_t doc_e_step(document* doc, fp_t* gamma, fp_t* phi,
     }
     if (LEFTOVER(model->num_topics, 0)) {
         __m256fp gams = _mm256_maskload(gamma + KK, KMASK);
-        __m256fp digams = digamma_vec_mask(gams, KMASK);
+
+        __m256fp dig_unmasked = digamma_vec(gams);
+        __m256fp digams = _mm256_and(dig_unmasked, _mm256_castsi256(KMASK));
 
         gamma_accs = _mm256_add(gamma_accs, gams);
         alpha_accs = _mm256_add(alpha_accs, digams);
@@ -82,18 +84,113 @@ fp_t doc_e_step(document* doc, fp_t* gamma, fp_t* phi,
     ss->alpha_suffstats += first(alpha_totals);
     ss->alpha_suffstats -= model->num_topics * digamma(gamma_sum);
 
-
     //Update beta.
-    for (n = 0; n < doc->length; n++)
+    // <CC> Tile by 4 on n (two words in parallel)
+    int tiling_factor = 4;
+    for (n = 0; n + tiling_factor - 1 < doc->length; n+=tiling_factor)
+    {
+        int di1 = doc->words[n] * model->num_topics;
+        int di2 = doc->words[n + 1] * model->num_topics;
+        int di3 = doc->words[n + 2] * model->num_topics;
+        int di4 = doc->words[n + 3] * model->num_topics;
+
+        int ni1 = n * model->num_topics;
+        int ni2 = (n + 1) * model->num_topics;
+        int ni3 = (n + 2) * model->num_topics;
+        int ni4 = (n + 3) * model->num_topics;
+
+        __m256fp doc_counts1 = _mm256_set1(doc->counts[n]);
+        __m256fp doc_counts2 = _mm256_set1(doc->counts[n + 1]);
+        __m256fp doc_counts3 = _mm256_set1(doc->counts[n + 2]);
+        __m256fp doc_counts4 = _mm256_set1(doc->counts[n + 3]);
+
+        for (k = 0; k < KK; k += STRIDE)
+        {
+            __m256fp ct = _mm256_loadu(ss->class_total + k);
+
+            //ss->class_word[di + k] += doc->counts[n]*phi[ni + k];
+            //ss->class_total[k] += doc->counts[n]*phi[ni + k];
+
+            // Tile 1
+            __m256fp cw1 = _mm256_loadu(ss->class_word + di1 + k);
+            __m256fp ph1 = _mm256_loadu(phi + ni1 + k);
+            cw1 = _mm256_fmadd(doc_counts1, ph1, cw1);
+            ct = _mm256_fmadd(doc_counts1, ph1, ct);
+            _mm256_storeu(ss->class_word + di1 + k, cw1);
+
+            // Tile 2
+            __m256fp cw2 = _mm256_loadu(ss->class_word + di2 + k);
+            __m256fp ph2 = _mm256_loadu(phi + ni2 + k);
+            cw2 = _mm256_fmadd(doc_counts2, ph2, cw2);
+            ct = _mm256_fmadd(doc_counts2, ph2, ct);
+            _mm256_storeu(ss->class_word + di2 + k, cw2);
+
+            // Tile 3
+            __m256fp cw3 = _mm256_loadu(ss->class_word + di3 + k);
+            __m256fp ph3 = _mm256_loadu(phi + ni3 + k);
+            cw3 = _mm256_fmadd(doc_counts3, ph3, cw3);
+            ct = _mm256_fmadd(doc_counts3, ph3, ct);
+            _mm256_storeu(ss->class_word + di3 + k, cw3);
+
+            // Tile 4
+            __m256fp cw4 = _mm256_loadu(ss->class_word + di4 + k);
+            __m256fp ph4 = _mm256_loadu(phi + ni4 + k);
+            cw4 = _mm256_fmadd(doc_counts4, ph4, cw4);
+            ct = _mm256_fmadd(doc_counts4, ph4, ct);
+            _mm256_storeu(ss->class_word + di4 + k, cw4);
+
+            _mm256_storeu(ss->class_total + k, ct);
+        }
+ 
+        if (LEFTOVER(model->num_topics, 0)) 
+        {
+            __m256fp ct = _mm256_maskload(ss->class_total + KK, KMASK);
+
+            //ss->class_word[di + k] += doc->counts[n]*phi[ni + k];
+            //ss->class_total[k] += doc->counts[n]*phi[ni + k];
+
+            // Tile 1
+            __m256fp cw1 = _mm256_maskload(ss->class_word + di1 + KK, KMASK);
+            __m256fp ph1 = _mm256_maskload(phi + ni1 + KK, KMASK);
+            cw1 = _mm256_fmadd(doc_counts1, ph1, cw1);
+            ct = _mm256_fmadd(doc_counts1, ph1, ct);
+            _mm256_maskstore(ss->class_word + di1 + KK, KMASK, cw1);
+
+            // Tile 2
+            __m256fp cw2 = _mm256_maskload(ss->class_word + di2 + KK, KMASK);
+            __m256fp ph2 = _mm256_maskload(phi + ni2 + KK, KMASK);
+            cw2 = _mm256_fmadd(doc_counts2, ph2, cw2);
+            ct = _mm256_fmadd(doc_counts2, ph2, ct);
+            _mm256_maskstore(ss->class_word + di2 + KK, KMASK, cw2);
+
+            // Tile 3
+            __m256fp cw3 = _mm256_maskload(ss->class_word + di3 + KK, KMASK);
+            __m256fp ph3 = _mm256_maskload(phi + ni3 + KK, KMASK);
+            cw3 = _mm256_fmadd(doc_counts3, ph3, cw3);
+            ct = _mm256_fmadd(doc_counts3, ph3, ct);
+            _mm256_maskstore(ss->class_word + di3 + KK, KMASK, cw3);
+
+            // Tile 4
+            __m256fp cw4 = _mm256_maskload(ss->class_word + di4 + KK, KMASK);
+            __m256fp ph4 = _mm256_maskload(phi + ni4 + KK, KMASK);
+            cw4 = _mm256_fmadd(doc_counts4, ph4, cw4);
+            ct = _mm256_fmadd(doc_counts4, ph4, ct);
+            _mm256_maskstore(ss->class_word + di4 + KK, KMASK, cw4);
+
+            _mm256_maskstore(ss->class_total + KK, KMASK, ct);
+        }
+    }
+
+    for (; n < doc->length; n++)
     {
         int di = doc->words[n] * model->num_topics;
         int ni = n * model->num_topics;
         __m256fp doc_counts = _mm256_set1(doc->counts[n]);
         for (k = 0; k < KK; k += STRIDE)
         {
-            __m256fp cw = _mm256_load(ss->class_word + di + k);
-            __m256fp ct = _mm256_load(ss->class_total + k);
-            __m256fp ph = _mm256_load(phi + ni + k);
+            __m256fp cw = _mm256_loadu(ss->class_word + di + k);
+            __m256fp ct = _mm256_loadu(ss->class_total + k);
+            __m256fp ph = _mm256_loadu(phi + ni + k);
 
             //ss->class_word[di + k] += doc->counts[n]*phi[ni + k];
             cw = _mm256_fmadd(doc_counts, ph, cw);
@@ -101,12 +198,13 @@ fp_t doc_e_step(document* doc, fp_t* gamma, fp_t* phi,
             //ss->class_total[k] += doc->counts[n]*phi[ni + k];
             ct = _mm256_fmadd(doc_counts, ph, ct);
 
-            _mm256_store(ss->class_word + di + k, cw);
-            _mm256_store(ss->class_total + k, ct);
+            _mm256_storeu(ss->class_word + di + k, cw);
+            _mm256_storeu(ss->class_total + k, ct);
         }
 
         if (LEFTOVER(model->num_topics, 0)) {
             __m256fp cw = _mm256_maskload(ss->class_word + di + KK, KMASK);
+
             __m256fp ct = _mm256_maskload(ss->class_total + KK, KMASK);
             __m256fp ph = _mm256_maskload(phi + ni + KK, KMASK);
 
@@ -138,14 +236,13 @@ void run_em(char* start, char* directory, corpus* corpus)
     fp_t **var_gamma, *phi;
 
     // Gamma variational parameter for each doc and for each topic.
-    // This doesn't need to be aligned
-    var_gamma = malloc(sizeof(fp_t*)*(corpus->num_docs));
+    var_gamma = _mm_malloc(sizeof(fp_t*)*(corpus->num_docs), ALIGNMENT);
     for (d = 0; d < corpus->num_docs; d++)
-        var_gamma[d] = _mm_malloc(sizeof(fp_t) * NTOPICS, 32);
+        var_gamma[d] = _mm_malloc(sizeof(fp_t) * NTOPICS, ALIGNMENT);
 
     // Phi variational parameter for each term in the vocabulary and for each topic.
     int max_length = max_corpus_length(corpus);
-    phi = _mm_malloc(sizeof(fp_t) * max_length * NTOPICS, 32);
+    phi = _mm_malloc(sizeof(fp_t) * max_length * NTOPICS, ALIGNMENT);
 
     // initialize model
     char filename[1000];
@@ -162,9 +259,6 @@ void run_em(char* start, char* directory, corpus* corpus)
         ss = new_lda_suffstats(model);
     }
 
-    sprintf(filename,"%s/000",directory);
-    save_lda_model(model, filename, max_length);
-
 
     // run expectation maximization
     int var_iter = 0;
@@ -209,11 +303,10 @@ void run_em(char* start, char* directory, corpus* corpus)
     timer_manual_increment(EM_CONVERGE, var_iter);
 
     // output the final model
-    sprintf(filename,"%s/final",directory);
-    save_lda_model(model, filename, max_length);
+    // sprintf(filename,"%s/final",directory);
+    // save_lda_model(model, filename, max_length);
     sprintf(filename,"%s/final.gamma",directory);
     save_gamma(filename, var_gamma, corpus->num_docs, model->num_topics);
 
     // <BG>: output the word assignments (for visualization) were removed
 }
-
diff --git a/fast-lda/lda-inference.c b/fast-lda/lda-inference.c
index ccbe01a..309aebd 100644
--- a/fast-lda/lda-inference.c
+++ b/fast-lda/lda-inference.c
@@ -26,11 +26,26 @@ fp_t lda_inference(document* doc, lda_model* model, fp_t* var_gamma, fp_t* phi)
     __m256i rem;
     STRIDE_SPLIT(model->num_topics, 0, &kk, &rem);
 
+    int kk1;
+    __m256i rem1;
+    STRIDE_SPLIT(model->num_topics, 1, &kk1, &rem1);
+
     fp_t converged = 1;
-    fp_t phisum = 0, likelihood = 0;
-    fp_t likelihood_old = 0, oldphi[model->num_topics];
+    fp_t likelihood = 0;
+    fp_t likelihood_old = 0;
     int k, n, var_iter;
-    fp_t *digamma_gam = _mm_malloc(sizeof(fp_t)*model->num_topics, 32);
+
+    fp_t oldphi0[model->num_topics];
+    fp_t oldphi1[model->num_topics];
+    fp_t oldphi2[model->num_topics];
+    fp_t oldphi3[model->num_topics];
+
+    fp_t phisum0 = 0;
+    fp_t phisum1 = 0;
+    fp_t phisum2 = 0;
+    fp_t phisum3 = 0;
+
+    fp_t digamma_gam[model->num_topics];
 
     timer rdtsc = start_timer(LDA_INFERENCE);
 
@@ -53,32 +68,266 @@ fp_t lda_inference(document* doc, lda_model* model, fp_t* var_gamma, fp_t* phi)
 
     var_iter = 0;
 
+    int rest = doc->length % 4;
+
     while ((converged > VAR_CONVERGED) &&
      ((var_iter < VAR_MAX_ITER) || (VAR_MAX_ITER == -1)))
     {
        var_iter++;
        // Update equation (16) for variational phi for each topic and word.
-       for (n = 0; n < doc->length; n++)
+       for (n = 0; n < doc->length - rest; n += 4)
        {
             // <FL> First backup phi entirely
-            memcpy(oldphi, phi + n * model->num_topics, sizeof(fp_t) * model->num_topics);
+            memcpy(oldphi0, phi + (n + 0) * model->num_topics, sizeof(fp_t) * model->num_topics);
+            memcpy(oldphi1, phi + (n + 1) * model->num_topics, sizeof(fp_t) * model->num_topics);
+            memcpy(oldphi2, phi + (n + 2) * model->num_topics, sizeof(fp_t) * model->num_topics);
+            memcpy(oldphi3, phi + (n + 3) * model->num_topics, sizeof(fp_t) * model->num_topics);
+
             // <BG, SS> Moved if else initialization of phisum outside of the loop
+            phi[(n + 0) * model->num_topics + 0] = digamma_gam[0] + model->log_prob_w_doc[(n + 0) * model->num_topics + 0];
+            phi[(n + 1) * model->num_topics + 0] = digamma_gam[0] + model->log_prob_w_doc[(n + 1) * model->num_topics + 0];
+            phi[(n + 2) * model->num_topics + 0] = digamma_gam[0] + model->log_prob_w_doc[(n + 2) * model->num_topics + 0];
+            phi[(n + 3) * model->num_topics + 0] = digamma_gam[0] + model->log_prob_w_doc[(n + 3) * model->num_topics + 0];
+
+            // phisum0 = phi[(n + 0) * model->num_topics + 0];
+            // phisum1 = phi[(n + 1) * model->num_topics + 0];
+            // phisum2 = phi[(n + 2) * model->num_topics + 0];
+            // phisum3 = phi[(n + 3) * model->num_topics + 0];
+
+            for (k = 1; k < kk1; k += STRIDE)
+            {
+                //phi[n * model->num_topics + k] = digamma_gam[k] + model->log_prob_w_doc[n * model->num_topics + k];
+                __m256fp dg = _mm256_loadu(digamma_gam + k);
+                __m256fp lpwd0 = _mm256_loadu(model->log_prob_w_doc + ((n + 0) * model->num_topics + k));
+                __m256fp lpwd1 = _mm256_loadu(model->log_prob_w_doc + ((n + 1) * model->num_topics + k));
+                __m256fp lpwd2 = _mm256_loadu(model->log_prob_w_doc + ((n + 2) * model->num_topics + k));
+                __m256fp lpwd3 = _mm256_loadu(model->log_prob_w_doc + ((n + 3) * model->num_topics + k));
+
+                __m256fp ph0 = _mm256_add(dg, lpwd0);
+                __m256fp ph1 = _mm256_add(dg, lpwd1);
+                __m256fp ph2 = _mm256_add(dg, lpwd2);
+                __m256fp ph3 = _mm256_add(dg, lpwd3);
+
+                _mm256_storeu(phi + ((n + 0) * model->num_topics + k), ph0);
+                _mm256_storeu(phi + ((n + 1) * model->num_topics + k), ph1);
+                _mm256_storeu(phi + ((n + 2) * model->num_topics + k), ph2);
+                _mm256_storeu(phi + ((n + 3) * model->num_topics + k), ph3);
+            }
 
+            if (LEFTOVER(model->num_topics, 1)) {
+                __m256fp dg = _mm256_maskload(digamma_gam + k, rem1);
+                __m256fp lpwd0 = _mm256_maskload(model->log_prob_w_doc + ((n + 0) * model->num_topics + k), rem1);
+                __m256fp lpwd1 = _mm256_maskload(model->log_prob_w_doc + ((n + 1) * model->num_topics + k), rem1);
+                __m256fp lpwd2 = _mm256_maskload(model->log_prob_w_doc + ((n + 2) * model->num_topics + k), rem1);
+                __m256fp lpwd3 = _mm256_maskload(model->log_prob_w_doc + ((n + 3) * model->num_topics + k), rem1);
+
+                __m256fp ph0 = _mm256_add(dg, lpwd0);
+                __m256fp ph1 = _mm256_add(dg, lpwd1);
+                __m256fp ph2 = _mm256_add(dg, lpwd2);
+                __m256fp ph3 = _mm256_add(dg, lpwd3);
+
+                _mm256_maskstore(phi + ((n + 0) * model->num_topics + k), rem1, ph0);
+                _mm256_maskstore(phi + ((n + 1) * model->num_topics + k), rem1, ph1);
+                _mm256_maskstore(phi + ((n + 2) * model->num_topics + k), rem1, ph2);
+                _mm256_maskstore(phi + ((n + 3) * model->num_topics + k), rem1, ph3);
+            }
+
+            
+            int kkSTRIDE;
+            __m256i remSTRIDE;
+            STRIDE_SPLIT(model->num_topics, STRIDE, &kkSTRIDE, &remSTRIDE);
+
+            __m256fp v_phsum0 = _mm256_loadu(phi + (n * model->num_topics));
+            __m256fp v_phsum1 = _mm256_loadu(phi + ((n + 1) * model->num_topics));
+            __m256fp v_phsum2 = _mm256_loadu(phi + ((n + 2) * model->num_topics));
+            __m256fp v_phsum3 = _mm256_loadu(phi + ((n + 3) * model->num_topics));
+
+
+            for (k = STRIDE; k < kkSTRIDE; k += STRIDE)
+            {
+                __m256fp ph0 = _mm256_loadu(phi + (n * model->num_topics + k));
+                __m256fp ph1 = _mm256_loadu(phi + ((n + 1) * model->num_topics + k));
+                __m256fp ph2 = _mm256_loadu(phi + ((n + 2) * model->num_topics + k));
+                __m256fp ph3 = _mm256_loadu(phi + ((n + 3) * model->num_topics + k));
+
+                v_phsum0 = log_sum_vec(v_phsum0, ph0);
+                v_phsum1 = log_sum_vec(v_phsum1, ph1);
+                v_phsum2 = log_sum_vec(v_phsum2, ph2);
+                v_phsum3 = log_sum_vec(v_phsum3, ph3);
+
+            }
+            if (LEFTOVER(model->num_topics, STRIDE))
+            {
+                __m256fp ph0 = _mm256_maskload(phi + (n * model->num_topics + k), remSTRIDE);
+                __m256fp ph1 = _mm256_maskload(phi + ((n + 1) * model->num_topics + k), remSTRIDE);
+                __m256fp ph2 = _mm256_maskload(phi + ((n + 2) * model->num_topics + k), remSTRIDE);
+                __m256fp ph3 = _mm256_maskload(phi + ((n + 3) * model->num_topics + k), remSTRIDE);
+
+                v_phsum0 = log_sum_vec_masked(v_phsum0, ph0, remSTRIDE);
+                v_phsum1 = log_sum_vec_masked(v_phsum1, ph1, remSTRIDE);
+                v_phsum2 = log_sum_vec_masked(v_phsum2, ph2, remSTRIDE);
+                v_phsum3 = log_sum_vec_masked(v_phsum3, ph3, remSTRIDE);
+            }
+            
+             // <CC> Finilized the phi_sum
+
+            double ph_fisrt_half_sum0 = log_sum(((fp_t *)(&v_phsum0))[0], ((fp_t *)(&v_phsum0))[1]);
+            double ph_second_half_sum0 = log_sum(((fp_t *)(&v_phsum0))[2], ((fp_t *)(&v_phsum0))[3]);
+            phisum0 = log_sum(ph_fisrt_half_sum0, ph_second_half_sum0); 
+
+
+            double ph_fisrt_half_sum1 = log_sum(((fp_t *)(&v_phsum1))[0], ((fp_t *)(&v_phsum1))[1]);
+            double ph_second_half_sum1 = log_sum(((fp_t *)(&v_phsum1))[2], ((fp_t *)(&v_phsum1))[3]);
+            phisum1 = log_sum(ph_fisrt_half_sum1, ph_second_half_sum1);    
+
+            double ph_fisrt_half_sum2 = log_sum(((fp_t *)(&v_phsum2))[0], ((fp_t *)(&v_phsum2))[1]);
+            double ph_second_half_sum2 = log_sum(((fp_t *)(&v_phsum2))[2], ((fp_t *)(&v_phsum2))[3]);
+            phisum2 = log_sum(ph_fisrt_half_sum2, ph_second_half_sum2);    
+
+            double ph_fisrt_half_sum3 = log_sum(((fp_t *)(&v_phsum3))[0], ((fp_t *)(&v_phsum3))[1]);
+            double ph_second_half_sum3 = log_sum(((fp_t *)(&v_phsum3))[2], ((fp_t *)(&v_phsum3))[3]);
+            phisum3 = log_sum(ph_fisrt_half_sum3, ph_second_half_sum3);    
+   
+
+            __m256fp doc_counts0 = _mm256_set1(doc->counts[(n + 0)]);
+            __m256fp doc_counts1 = _mm256_set1(doc->counts[(n + 1)]);
+            __m256fp doc_counts2 = _mm256_set1(doc->counts[(n + 2)]);
+            __m256fp doc_counts3 = _mm256_set1(doc->counts[(n + 3)]);
+
+            __m256fp ph_sum0 = _mm256_set1(phisum0);
+            __m256fp ph_sum1 = _mm256_set1(phisum1);
+            __m256fp ph_sum2 = _mm256_set1(phisum2);
+            __m256fp ph_sum3 = _mm256_set1(phisum3);
+
+            //Update equation (17) for variational gamma for each topic
             for (k = 0; k < kk; k += STRIDE)
             {
+                // Write the final value of the update for phi.
+                __m256fp ph0 = _mm256_loadu(phi + ((n + 0) * model->num_topics + k));
+                __m256fp ph1 = _mm256_loadu(phi + ((n + 1) * model->num_topics + k));
+                __m256fp ph2 = _mm256_loadu(phi + ((n + 2) * model->num_topics + k));
+                __m256fp ph3 = _mm256_loadu(phi + ((n + 3) * model->num_topics + k));
+
+                __m256fp pho0 = _mm256_loadu(oldphi0 + k);
+                __m256fp pho1 = _mm256_loadu(oldphi1 + k);
+                __m256fp pho2 = _mm256_loadu(oldphi2 + k);
+                __m256fp pho3 = _mm256_loadu(oldphi3 + k);
+
+                __m256fp vg = _mm256_loadu(var_gamma + k);
+
+                // phi[n * model->num_topics + k] = exp(phi[n * model->num_topics + k] - phisum);
+                __m256fp ph_diff_sum0 = _mm256_sub(ph0, ph_sum0);
+                __m256fp ph_diff_sum1 = _mm256_sub(ph1, ph_sum1);
+                __m256fp ph_diff_sum2 = _mm256_sub(ph2, ph_sum2);
+                __m256fp ph_diff_sum3 = _mm256_sub(ph3, ph_sum3);
+
+                ph0 = _mm256_exp(ph_diff_sum0);
+                ph1 = _mm256_exp(ph_diff_sum1);
+                ph2 = _mm256_exp(ph_diff_sum2);
+                ph3 = _mm256_exp(ph_diff_sum3);
+
+                _mm256_storeu(phi + ((n + 0) * model->num_topics + k), ph0);
+                _mm256_storeu(phi + ((n + 1) * model->num_topics + k), ph1);
+                _mm256_storeu(phi + ((n + 2) * model->num_topics + k), ph2);
+                _mm256_storeu(phi + ((n + 3) * model->num_topics + k), ph3);
+
+                // var_gamma[k] = var_gamma[k] + doc->counts[n]*(phi[n * model->num_topics + k] - oldphi[k]);
+                __m256fp ph_diff_old0 = _mm256_sub(ph0, pho0);
+                __m256fp ph_diff_old1 = _mm256_sub(ph1, pho1);
+                __m256fp ph_diff_old2 = _mm256_sub(ph2, pho2);
+                __m256fp ph_diff_old3 = _mm256_sub(ph3, pho3);
+
+                // <FL> Not the best but var_gamma is one vector and we have 4 other ones here.
+                vg = _mm256_fmadd(doc_counts0, ph_diff_old0, vg);
+                vg = _mm256_fmadd(doc_counts1, ph_diff_old1, vg);
+                vg = _mm256_fmadd(doc_counts2, ph_diff_old2, vg);
+                vg = _mm256_fmadd(doc_counts3, ph_diff_old3, vg);
+
+                _mm256_storeu(var_gamma + k, vg);
+
+                // !!! a lot of extra digamma's here because of how we're computing it
+                // !!! but its more automatically updated too.
+                // digamma_gam[k] = digamma(var_gamma[k]);
+                __m256fp dg = digamma_vec(vg);
+                _mm256_storeu(digamma_gam + k, dg);
+            }
+
+            if (LEFTOVER(model->num_topics, 0))
+            {
+                // Write the final value of the update for phi.
+                __m256fp ph0 = _mm256_maskload(phi + ((n + 0) * model->num_topics + k), rem);
+                __m256fp ph1 = _mm256_maskload(phi + ((n + 1) * model->num_topics + k), rem);
+                __m256fp ph2 = _mm256_maskload(phi + ((n + 2) * model->num_topics + k), rem);
+                __m256fp ph3 = _mm256_maskload(phi + ((n + 3) * model->num_topics + k), rem);
+
+                __m256fp pho0 = _mm256_maskload(oldphi0 + k, rem);
+                __m256fp pho1 = _mm256_maskload(oldphi1 + k, rem);
+                __m256fp pho2 = _mm256_maskload(oldphi2 + k, rem);
+                __m256fp pho3 = _mm256_maskload(oldphi3 + k, rem);
+
+                __m256fp vg = _mm256_maskload(var_gamma + k, rem);
+
+                // phi[n * model->num_topics + k] = exp(phi[n * model->num_topics + k] - phisum);
+                __m256fp ph_diff_sum0 = _mm256_sub(ph0, ph_sum0);
+                __m256fp ph_diff_sum1 = _mm256_sub(ph1, ph_sum1);
+                __m256fp ph_diff_sum2 = _mm256_sub(ph2, ph_sum2);
+                __m256fp ph_diff_sum3 = _mm256_sub(ph3, ph_sum3);
+
+                ph0 = _mm256_exp(ph_diff_sum0);
+                ph1 = _mm256_exp(ph_diff_sum1);
+                ph2 = _mm256_exp(ph_diff_sum2);
+                ph3 = _mm256_exp(ph_diff_sum3);
+
+                _mm256_maskstore(phi + ((n + 0) * model->num_topics + k), rem, ph0);
+                _mm256_maskstore(phi + ((n + 1) * model->num_topics + k), rem, ph1);
+                _mm256_maskstore(phi + ((n + 2) * model->num_topics + k), rem, ph2);
+                _mm256_maskstore(phi + ((n + 3) * model->num_topics + k), rem, ph3);
+
+                // var_gamma[k] = var_gamma[k] + doc->counts[n]*(phi[n * model->num_topics + k] - oldphi[k]);
+                __m256fp ph_diff_old0 = _mm256_sub(ph0, pho0);
+                __m256fp ph_diff_old1 = _mm256_sub(ph1, pho1);
+                __m256fp ph_diff_old2 = _mm256_sub(ph2, pho2);
+                __m256fp ph_diff_old3 = _mm256_sub(ph3, pho3);
+
+                vg = _mm256_fmadd(doc_counts0, ph_diff_old0, vg);
+                vg = _mm256_fmadd(doc_counts1, ph_diff_old1, vg);
+                vg = _mm256_fmadd(doc_counts2, ph_diff_old2, vg);
+                vg = _mm256_fmadd(doc_counts3, ph_diff_old3, vg);
+
+                _mm256_maskstore(var_gamma + k, rem, vg);
+
+                // !!! a lot of extra digamma's here because of how we're computing it
+                // !!! but its more automatically updated too.
+                // digamma_gam[k] = digamma(var_gamma[k]);
+                __m256fp dg = digamma_vec(vg);
+                _mm256_maskstore(digamma_gam + k, rem, dg);
+            }
+
+        }
+        // Now do the remaining doc->length % 4 rows, one by one.
+        for (; n < doc->length ; n++) {
+
+            // <FL> First backup phi entirely
+            memcpy(oldphi0, phi + n * model->num_topics, sizeof(fp_t) * model->num_topics);
+            // <BG, SS> Moved if else initialization of phisum outside of the loop
+            phi[n * model->num_topics + 0] = digamma_gam[0] + model->log_prob_w[doc->words[n] * model->num_topics + 0];
+            phisum0 = phi[n * model->num_topics + 0];
+
+            for (k = 1; k < kk1; k += STRIDE)
+            {
                 //phi[n * model->num_topics + k] = digamma_gam[k] + model->log_prob_w_doc[n * model->num_topics + k];
-                __m256fp dg = _mm256_load(digamma_gam + k);
-                __m256fp lpwd = _mm256_load(model->log_prob_w_doc + (n * model->num_topics + k));
+                __m256fp dg = _mm256_loadu(digamma_gam + k);
+                __m256fp lpwd = _mm256_loadu(model->log_prob_w_doc + (n * model->num_topics + k));
 
                 __m256fp ph = _mm256_add(dg, lpwd);
-                _mm256_store(phi + (n * model->num_topics + k), ph);
+                _mm256_storeu(phi + (n * model->num_topics + k), ph);
             }
             if (LEFTOVER(model->num_topics, 1)) {
-                __m256fp dg = _mm256_maskload(digamma_gam + k, rem);
-                __m256fp lpwd = _mm256_maskload(model->log_prob_w_doc + (n * model->num_topics + k), rem);
+                __m256fp dg = _mm256_maskload(digamma_gam + k, rem1);
+                __m256fp lpwd = _mm256_maskload(model->log_prob_w_doc + (n * model->num_topics + k), rem1);
 
                 __m256fp ph = _mm256_add(dg, lpwd);
-                _mm256_maskstore(phi + (n * model->num_topics + k), rem, ph);
+                _mm256_maskstore(phi + (n * model->num_topics + k), rem1, ph);
             }
 
 
@@ -86,12 +335,11 @@ fp_t lda_inference(document* doc, lda_model* model, fp_t* var_gamma, fp_t* phi)
             __m256i remSTRIDE;
             STRIDE_SPLIT(model->num_topics, STRIDE, &kkSTRIDE, &remSTRIDE);
 
-            __m256fp v_phsum = _mm256_load(phi + (n * model->num_topics));
+            __m256fp v_phsum = _mm256_loadu(phi + (n * model->num_topics));
 
             for (k = STRIDE; k < kkSTRIDE; k += STRIDE)
             {
-
-                __m256fp ph = _mm256_load(phi + (n * model->num_topics + k));
+                __m256fp ph = _mm256_loadu(phi + (n * model->num_topics + k));
                 v_phsum = log_sum_vec(v_phsum, ph);
             }
             if (LEFTOVER(model->num_topics, STRIDE))
@@ -104,42 +352,41 @@ fp_t lda_inference(document* doc, lda_model* model, fp_t* var_gamma, fp_t* phi)
             double some_sum0 = log_sum(((fp_t *)(&v_phsum))[0], ((fp_t *)(&v_phsum))[1]);
             double some_sum1 = log_sum(((fp_t *)(&v_phsum))[2], ((fp_t *)(&v_phsum))[3]);
 
-            phisum = log_sum(some_sum0, some_sum1);
+            phisum0 = log_sum(some_sum0, some_sum1);
 
             __m256fp doc_counts = _mm256_set1(doc->counts[n]);
-            __m256fp ph_sum = _mm256_set1(phisum);
+            __m256fp ph_sum = _mm256_set1(phisum0);
 
             //Update equation (17) for variational gamma for each topic
             for (k = 0; k < kk; k += STRIDE)
             {
                 // Write the final value of the update for phi.
-                __m256fp ph = _mm256_load(phi + (n * model->num_topics + k));
-                __m256fp pho = _mm256_load(oldphi + k);
-                __m256fp vg = _mm256_load(var_gamma + k);
+                __m256fp ph = _mm256_loadu(phi + (n * model->num_topics + k));
+                __m256fp pho = _mm256_loadu(oldphi0 + k);
+                __m256fp vg = _mm256_loadu(var_gamma + k);
 
                 // phi[n * model->num_topics + k] = exp(phi[n * model->num_topics + k] - phisum);
                 __m256fp ph_diff_sum = _mm256_sub(ph, ph_sum);
                 ph = _mm256_exp(ph_diff_sum);
-                _mm256_store(phi + (n * model->num_topics + k), ph);
+                _mm256_storeu(phi + (n * model->num_topics + k), ph);
 
                 // var_gamma[k] = var_gamma[k] + doc->counts[n]*(phi[n * model->num_topics + k] - oldphi[k]);
                 __m256fp ph_diff_old = _mm256_sub(ph, pho);
-                __m256fp ph_diff_times_doc_counts = _mm256_mul(doc_counts, ph_diff_old);
-                vg = _mm256_add(vg, ph_diff_times_doc_counts);
-                _mm256_store(var_gamma + k, vg);
+                vg = _mm256_fmadd(doc_counts, ph_diff_old, vg);
+                _mm256_storeu(var_gamma + k, vg);
 
                 // !!! a lot of extra digamma's here because of how we're computing it
                 // !!! but its more automatically updated too.
                 // digamma_gam[k] = digamma(var_gamma[k]);
                 __m256fp dg = digamma_vec(vg);
-                _mm256_store(digamma_gam + k, dg);
+                _mm256_storeu(digamma_gam + k, dg);
             }
 
             if (LEFTOVER(model->num_topics, 0))
             {
                 // Write the final value of the update for phi.
                 __m256fp ph = _mm256_maskload(phi + (n * model->num_topics + k), rem);
-                __m256fp pho = _mm256_maskload(oldphi + k, rem);
+                __m256fp pho = _mm256_maskload(oldphi0 + k, rem);
                 __m256fp vg = _mm256_maskload(var_gamma + k, rem);
 
                 // phi[n * model->num_topics + k] = exp(phi[n * model->num_topics + k] - phisum);
@@ -149,8 +396,7 @@ fp_t lda_inference(document* doc, lda_model* model, fp_t* var_gamma, fp_t* phi)
 
                 // var_gamma[k] = var_gamma[k] + doc->counts[n]*(phi[n * model->num_topics + k] - oldphi[k]);
                 __m256fp ph_diff_old = _mm256_sub(ph, pho);
-                __m256fp ph_diff_times_doc_counts = _mm256_mul(doc_counts, ph_diff_old);
-                vg = _mm256_add(vg, ph_diff_times_doc_counts);
+                vg = _mm256_fmadd(doc_counts, ph_diff_old, vg);
                 _mm256_maskstore(var_gamma + k, rem, vg);
 
                 // !!! a lot of extra digamma's here because of how we're computing it
@@ -159,9 +405,9 @@ fp_t lda_inference(document* doc, lda_model* model, fp_t* var_gamma, fp_t* phi)
                 __m256fp dg = digamma_vec(vg);
                 _mm256_maskstore(digamma_gam + k, rem, dg);
             }
-
         }
 
+
         likelihood = compute_likelihood(doc, model, phi, var_gamma);
         assert(!isnan(likelihood));
         converged = (likelihood_old - likelihood) / likelihood_old;
@@ -180,10 +426,7 @@ fp_t lda_inference(document* doc, lda_model* model, fp_t* var_gamma, fp_t* phi)
 fp_t compute_likelihood(document* doc, lda_model* model, fp_t* phi, fp_t* var_gamma)
 {
     fp_t likelihood = 0, digsum = 0, var_gamma_sum = 0;
-    fp_t *dig = _mm_malloc(sizeof(fp_t)*model->num_topics, 32);
-    __m256fp v_likelihood = _mm256_set1(0),
-
-    v_var_gamma_sum = _mm256_set1(0);
+    fp_t dig[model->num_topics];
 
     int k, n;
 
@@ -193,39 +436,76 @@ fp_t compute_likelihood(document* doc, lda_model* model, fp_t* phi, fp_t* var_ga
     timer rdtsc = start_timer(LIKELIHOOD);
 
     STRIDE_SPLIT(model->num_topics, 0, &kk, &leftover_mask);
+    int tiling_factor = 4;
+
+    __m256fp v_var_gamma_sum0 = _mm256_set1(0);
+    __m256fp v_var_gamma_sum1 = _mm256_set1(0);
+    __m256fp v_var_gamma_sum2 = _mm256_set1(0);
+    __m256fp v_var_gamma_sum3 = _mm256_set1(0);
 
-    for (k = 0; k < kk; k += STRIDE)
+    for (k = 0; k + (STRIDE * tiling_factor) - 1 < kk; k += STRIDE * tiling_factor)
     {
-       //dig[k] = digamma(var_gamma[k]);
-       __m256fp v_var_gamma = _mm256_load(var_gamma + k);
-       __m256fp v_dig = digamma_vec(v_var_gamma);
-       _mm256_store(dig + k, v_dig);
+        //dig[k] = digamma(var_gamma[k]);
+        //var_gamma_sum += var_gamma[k];
+        
+        // Tile 0
+       __m256fp v_var_gamma0 = _mm256_loadu(var_gamma + k + 0 * tiling_factor);
+       __m256fp v_dig0 = digamma_vec(v_var_gamma0);
+       _mm256_storeu(dig + k + 0 * tiling_factor, v_dig0);
+       v_var_gamma_sum0 = _mm256_add(v_var_gamma_sum0, v_var_gamma0);
+
+       // Tile 1
+        __m256fp v_var_gamma1 = _mm256_loadu(var_gamma + k + 1 * tiling_factor);
+       __m256fp v_dig1 = digamma_vec(v_var_gamma1);
+       _mm256_storeu(dig + k + 1 * tiling_factor, v_dig1);
+       v_var_gamma_sum1 = _mm256_add(v_var_gamma_sum1, v_var_gamma1);
+
+       //Tile 2
+        __m256fp v_var_gamma2 = _mm256_loadu(var_gamma + k + 2 * tiling_factor);
+       __m256fp v_dig2 = digamma_vec(v_var_gamma2);
+       _mm256_storeu(dig + k + 2 * tiling_factor, v_dig2);
+       v_var_gamma_sum2 = _mm256_add(v_var_gamma_sum2, v_var_gamma2);
+
+       // Tile3
+        __m256fp v_var_gamma3 = _mm256_loadu(var_gamma + k + 3 * tiling_factor);
+       __m256fp v_dig3 = digamma_vec(v_var_gamma3);
+       _mm256_storeu(dig + k + 3 * tiling_factor, v_dig3);
+       v_var_gamma_sum3 = _mm256_add(v_var_gamma_sum3, v_var_gamma3);
+    }
+
+    v_var_gamma_sum0 = _mm256_add(v_var_gamma_sum0, v_var_gamma_sum1);
+    v_var_gamma_sum0 = _mm256_add(v_var_gamma_sum0, v_var_gamma_sum2);
+    v_var_gamma_sum0 = _mm256_add(v_var_gamma_sum0, v_var_gamma_sum3);
 
-       //var_gamma_sum += var_gamma[k];
-       v_var_gamma_sum = _mm256_add(v_var_gamma_sum, v_var_gamma);
+    for(;k < kk;k += STRIDE)
+    {
+        __m256fp v_var_gamma0 = _mm256_loadu(var_gamma + k);
+        __m256fp v_dig0 = digamma_vec(v_var_gamma0);
+        _mm256_storeu(dig + k, v_dig0);
+        v_var_gamma_sum0 = _mm256_add(v_var_gamma_sum0, v_var_gamma0);
     }
 
     if (LEFTOVER(model->num_topics, 0)) {
-       //dig[k] = digamma(var_gamma[k]);
+       
        __m256fp v_var_gamma = _mm256_maskload(var_gamma + k, leftover_mask);
        __m256fp v_dig = digamma_vec(v_var_gamma);
        _mm256_maskstore(dig + k, leftover_mask, v_dig);
-
-       //var_gamma_sum += var_gamma[k];
-       v_var_gamma_sum = _mm256_add(v_var_gamma_sum, v_var_gamma);
+       v_var_gamma_sum0 = _mm256_add(v_var_gamma_sum0, v_var_gamma);
     }
 
-    __m256fp var_gamma_totals = hsum(v_var_gamma_sum);
-    var_gamma_sum = first(var_gamma_totals);
-
+    v_var_gamma_sum0 = hsum(v_var_gamma_sum0);
+    var_gamma_sum = first(v_var_gamma_sum0);
+    
     digsum = digamma(var_gamma_sum);
 
+
+
     __m256fp v_digsum = _mm256_set1(digsum);
     for (k = 0; k < kk; k += STRIDE) {
       //dig[k] = dig[k] - digsum;
-      __m256fp v_dig = _mm256_load(dig + k);
+      __m256fp v_dig = _mm256_loadu(dig + k);
       v_dig = _mm256_sub(v_dig, v_digsum);
-      _mm256_store(dig + k, v_dig);
+      _mm256_storeu(dig + k, v_dig);
     }
     if (LEFTOVER(model->num_topics, 0)) {
       //dig[k] = dig[k] - digsum;
@@ -239,41 +519,118 @@ fp_t compute_likelihood(document* doc, lda_model* model, fp_t* phi, fp_t* var_ga
     likelihood = lgamma(model->alpha * model -> num_topics)
                 - model -> num_topics * lgamma(model->alpha)
                 - (lgamma(var_gamma_sum));
-
-    // <SS>: TODO vectorized lgamma can be given entire array
+    
+    // <SS>: TODO vectorized lgamma can be given entire array 
     // Compute the log likelihood dependent on the variational parameters
     // as per equation (15).
-
-    __m256fp v_likelihood_2 = _mm256_set1(0);
+    
     fp_t alpha_m_1 = model->alpha - 1;
     __m256fp v_alpha_m_1 = _mm256_set1(alpha_m_1);
     __m256fp v_ones = _mm256_set1(1);
-    fp_t log_gamma_result[STRIDE];
-    for (k = 0; k < kk; k += STRIDE)
+    fp_t log_gamma_result0[STRIDE]; // array of length 4
+    fp_t log_gamma_result1[STRIDE];
+    fp_t log_gamma_result2[STRIDE];
+    fp_t log_gamma_result3[STRIDE];
+
+    __m256fp v_likelihood_k0 = _mm256_set1(0);
+    __m256fp v_likelihood_k1 = _mm256_set1(0);
+    __m256fp v_likelihood_k2 = _mm256_set1(0);
+    __m256fp v_likelihood_k3 = _mm256_set1(0);
+
+
+    for (k = 0; k + (STRIDE * tiling_factor) - 1 < kk; k += STRIDE * tiling_factor)
     {
         // likelihood += (model->alpha - 1)*dig[k]
         //             + lgamma(var_gamma[k])
         //             - (var_gamma[k] - 1)*dig[k];
-        vdLGamma(STRIDE, var_gamma + k, log_gamma_result);
-        __m256fp v_lgamma = _mm256_load(log_gamma_result);
+        
+        // Tile 0
+        vdLGamma(STRIDE, var_gamma + k + 0 * tiling_factor, log_gamma_result0);            
+        __m256fp v_lgamma0 = _mm256_loadu(log_gamma_result0);
 
-        __m256fp v_dig = _mm256_load(dig + k);
-        __m256fp v_t0 = _mm256_mul(v_dig, v_alpha_m_1);
+        __m256fp v_dig0 = _mm256_loadu(dig + k + 0 * tiling_factor);
+        __m256fp v_t00 = _mm256_mul(v_dig0, v_alpha_m_1);
 
-        __m256fp v_var_gamma = _mm256_load(var_gamma + k);
-        v_var_gamma = _mm256_sub(v_var_gamma, v_ones);
-        v_dig = _mm256_mul(v_var_gamma, v_dig);
+        __m256fp v_var_gamma0 = _mm256_loadu(var_gamma + k + 0 * tiling_factor);
+        v_var_gamma0 = _mm256_sub(v_var_gamma0, v_ones);
+        v_dig0 = _mm256_mul(v_var_gamma0, v_dig0);
+
+        v_likelihood_k0 = _mm256_add(v_likelihood_k0, v_lgamma0);
+        v_likelihood_k0 = _mm256_sub(v_likelihood_k0, v_dig0);
+        v_likelihood_k0 = _mm256_add(v_likelihood_k0, v_t00);
+
+        // Tile 1
+        vdLGamma(STRIDE, var_gamma + k + 1 * tiling_factor, log_gamma_result1); 
+        __m256fp v_lgamma1 = _mm256_loadu(log_gamma_result1);
+
+        __m256fp v_dig1 = _mm256_loadu(dig + k + 1 * tiling_factor);
+        __m256fp v_t01 = _mm256_mul(v_dig1, v_alpha_m_1);
+
+        __m256fp v_var_gamma1 = _mm256_loadu(var_gamma + k + 1 * tiling_factor);
+        v_var_gamma1 = _mm256_sub(v_var_gamma1, v_ones);
+        v_dig1 = _mm256_mul(v_var_gamma1, v_dig1);
+
+        v_likelihood_k1 = _mm256_add(v_likelihood_k1, v_lgamma1);
+        v_likelihood_k1 = _mm256_sub(v_likelihood_k1, v_dig1);
+        v_likelihood_k1 = _mm256_add(v_likelihood_k1, v_t01);
+
+        // Tile 2
+        vdLGamma(STRIDE, var_gamma + k + 2 * tiling_factor, log_gamma_result2); 
+        __m256fp v_lgamma2 = _mm256_loadu(log_gamma_result2);
+
+        __m256fp v_dig2 = _mm256_loadu(dig + k + 2 * tiling_factor);
+        __m256fp v_t02 = _mm256_mul(v_dig2, v_alpha_m_1);
 
+        __m256fp v_var_gamma2 = _mm256_loadu(var_gamma + k + 2 * tiling_factor);
+        v_var_gamma2 = _mm256_sub(v_var_gamma2, v_ones);
+        v_dig2 = _mm256_mul(v_var_gamma2, v_dig2);
 
-        v_likelihood_2 = _mm256_add(v_likelihood_2, v_lgamma);
-        v_likelihood_2 = _mm256_sub(v_likelihood_2, v_dig);
+        v_likelihood_k2 = _mm256_add(v_likelihood_k2, v_lgamma2);
+        v_likelihood_k2 = _mm256_sub(v_likelihood_k2, v_dig2);
+        v_likelihood_k2 = _mm256_add(v_likelihood_k2, v_t02);
 
-        v_likelihood_2 = _mm256_add(v_likelihood_2, v_t0);
+        // Tile 3
+        vdLGamma(STRIDE, var_gamma + k + 3 * tiling_factor, log_gamma_result3); 
+        __m256fp v_lgamma3 = _mm256_loadu(log_gamma_result3);
 
+        __m256fp v_dig3 = _mm256_loadu(dig + k + 3 * tiling_factor);
+        __m256fp v_t03 = _mm256_mul(v_dig3, v_alpha_m_1);
+
+        __m256fp v_var_gamma3 = _mm256_loadu(var_gamma + k + 3 * tiling_factor);
+        v_var_gamma3 = _mm256_sub(v_var_gamma3, v_ones);
+        v_dig3 = _mm256_mul(v_var_gamma3, v_dig3);
+
+        v_likelihood_k3 = _mm256_add(v_likelihood_k3, v_lgamma3);
+        v_likelihood_k3 = _mm256_sub(v_likelihood_k3, v_dig3);
+        v_likelihood_k3 = _mm256_add(v_likelihood_k3, v_t03);
+                  
+    }
+    v_likelihood_k0 = _mm256_add(v_likelihood_k0, v_likelihood_k1);
+    v_likelihood_k0 = _mm256_add(v_likelihood_k0, v_likelihood_k2);
+    v_likelihood_k0 = _mm256_add(v_likelihood_k0, v_likelihood_k3);
+
+    for(;k < kk; k += STRIDE)
+    {
+        // still vectorized, but not tiled
+        vdLGamma(STRIDE, var_gamma + k, log_gamma_result0);
+        __m256fp v_lgamma0 = _mm256_loadu(log_gamma_result0);
+
+        __m256fp v_dig0 = _mm256_loadu(dig + k);
+        __m256fp v_t00 = _mm256_mul(v_dig0, v_alpha_m_1);
+
+        __m256fp v_var_gamma0 = _mm256_loadu(var_gamma + k);
+        v_var_gamma0 = _mm256_sub(v_var_gamma0, v_ones);
+        v_dig0 = _mm256_mul(v_var_gamma0, v_dig0);
+
+        v_likelihood_k0 = _mm256_add(v_likelihood_k0, v_lgamma0);
+        v_likelihood_k0 = _mm256_sub(v_likelihood_k0, v_dig0);
+        v_likelihood_k0 = _mm256_add(v_likelihood_k0, v_t00);
+        
     }
     if (LEFTOVER(model->num_topics, 0)) {
-        vdLGamma(LEFTOVER(model->num_topics, 0), var_gamma + k, log_gamma_result);
-        __m256fp v_lgamma = _mm256_maskload(log_gamma_result, leftover_mask);
+
+        vdLGamma(LEFTOVER(model->num_topics, 0), var_gamma + k, log_gamma_result0);
+        __m256fp v_lgamma = _mm256_maskload(log_gamma_result0, leftover_mask);
 
         __m256fp v_dig = _mm256_maskload(dig + k, leftover_mask);
         __m256fp v_t0 = _mm256_mul(v_dig, v_alpha_m_1);
@@ -283,88 +640,182 @@ fp_t compute_likelihood(document* doc, lda_model* model, fp_t* phi, fp_t* var_ga
         v_dig = _mm256_mul(v_var_gamma, v_dig);
 
 
-        v_likelihood_2 = _mm256_add(v_likelihood_2, v_lgamma);
-        v_likelihood_2 = _mm256_sub(v_likelihood_2, v_dig);
-
-        v_likelihood_2 = _mm256_add(v_likelihood_2, v_t0);
+        v_likelihood_k0 = _mm256_add(v_likelihood_k0, v_lgamma);
+        v_likelihood_k0 = _mm256_sub(v_likelihood_k0, v_dig);
+        v_likelihood_k0 = _mm256_add(v_likelihood_k0, v_t0);
     }
-    v_likelihood_2 = hsum(v_likelihood_2);
-    likelihood += first(v_likelihood_2);
 
 
 
-    fp_t logcheck;
+    v_likelihood_k0 = hsum(v_likelihood_k0);
+
+    // <BG> += because there are multiple places where stuff gets added up to likelihood
+    likelihood += first(v_likelihood_k0);
+
+
+    __m256fp v_likelihood0 = _mm256_set1(0);
+    __m256fp v_likelihood1 = _mm256_set1(0);
+    __m256fp v_likelihood2 = _mm256_set1(0);
+    __m256fp v_likelihood3 = _mm256_set1(0);
+
+    __m256fp LOW = _mm256_set1(-100);
+
     // <CC> Swapped loop to have the strided access to the transposed
-    for (n = 0; n < doc->length; n++)
+    // 
+    // printf("%d\n", doc->length);
+    for (n = 0; n + tiling_factor - 1 < doc->length; n += tiling_factor)
     {
-        __m256fp v_doc_counts = _mm256_set1(doc->counts[n]);
+        __m256fp v_doc_counts0 = _mm256_set1(doc->counts[n+0]);
+        __m256fp v_doc_counts1 = _mm256_set1(doc->counts[n+1]);
+        __m256fp v_doc_counts2 = _mm256_set1(doc->counts[n+2]);
+        __m256fp v_doc_counts3 = _mm256_set1(doc->counts[n+3]);
 
         for (k = 0; k < kk; k += STRIDE)
         {
-            //t1 = dig[k];
-            __m256fp v_dig = _mm256_load(dig + k);
 
-            // t2 = phi[n * model->num_topics + k];
-            __m256fp v_phi = _mm256_load(phi + n * model->num_topics + k);
 
+            // t1 = dig[k];
+            // t2 = phi[n * model->num_topics + k];
             // t3 = log(t2)
-            __m256fp v_log_phi = _mm256_log(v_phi);
-
-            // <SS> seems like this can be removed, the original code
-            // has nothing going into else condition
             // t3 = MAX(t3, -100);
-            __m256fp LOW = _mm256_set1(-100);
-            v_log_phi = _mm256_max(v_log_phi, LOW);
-
             // t4 = model->log_prob_w_doc[n * model->num_topics + k];
-            // <SS> variable names are going out of hand -.-
-            __m256fp v_l_p_w_doc = _mm256_load(model->log_prob_w_doc + n * model->num_topics + k);
-
             // t5 = (t1 - t3 + t4);
-            __m256fp v_t5 = _mm256_add(_mm256_sub(v_dig, v_log_phi), v_l_p_w_doc);
-
             // t6 = doc->counts[n] * ( t2 *  t5);
-            __m256fp v_doc_likelihood = _mm256_mul(v_doc_counts, _mm256_mul(v_phi, v_t5));
-
             // likelihood = likelihood + t6;
-            v_likelihood = _mm256_add(v_likelihood, v_doc_likelihood);
-        }
-        if (LEFTOVER(model->num_topics, 0)) {
-            //t1 = dig[k];
-            __m256fp v_dig = _mm256_maskload(dig + k, leftover_mask);
 
-            // t2 = phi[n * model->num_topics + k];
-            __m256fp v_phi = _mm256_maskload(phi + n * model->num_topics + k, leftover_mask);
 
-            // t3 = log(t2)
-            __m256fp v_log_phi = _mm256_log(v_phi);
+            __m256fp v_dig = _mm256_loadu(dig + k);
 
-            // <SS> seems like this can be removed, the original code
-            // has nothing going into else condition
-            // t3 = MAX(t3, -100);
-            __m256fp LOW = _mm256_set1(-100);
-            v_log_phi = _mm256_max(v_log_phi, LOW);
+            // Tile 0
+            __m256fp v_phi0 = _mm256_loadu(phi + (n+0) * model->num_topics + k);
+            __m256fp v_log_phi0 = _mm256_log(v_phi0); 
+            v_log_phi0 = _mm256_max(v_log_phi0, LOW);
+            __m256fp v_l_p_w_doc0 = _mm256_loadu(model->log_prob_w_doc + (n+0) * model->num_topics + k);
+            __m256fp v_t50 = _mm256_add(_mm256_sub(v_dig, v_log_phi0), v_l_p_w_doc0);
+            __m256fp v_doc_likelihood0 = _mm256_mul(v_doc_counts0, _mm256_mul(v_phi0, v_t50));
 
-            // t4 = model->log_prob_w_doc[n * model->num_topics + k];
-            // <SS> variable names are going out of hand -.-
-            __m256fp v_l_p_w_doc = _mm256_maskload(model->log_prob_w_doc + n * model->num_topics + k, leftover_mask);
+            v_likelihood0 = _mm256_add(v_likelihood0, v_doc_likelihood0);
 
-            // t5 = (t1 - t3 + t4);
-            __m256fp v_t5 = _mm256_add(_mm256_sub(v_dig, v_log_phi), v_l_p_w_doc);
+            // Tile 1
+            __m256fp v_phi1 = _mm256_loadu(phi + (n+1) * model->num_topics + k);
+            __m256fp v_log_phi1 = _mm256_log(v_phi1); 
+            v_log_phi1 = _mm256_max(v_log_phi1, LOW);
+            __m256fp v_l_p_w_doc1 = _mm256_loadu(model->log_prob_w_doc + (n+1) * model->num_topics + k);
+            __m256fp v_t51 = _mm256_add(_mm256_sub(v_dig, v_log_phi1), v_l_p_w_doc1);
+            __m256fp v_doc_likelihood1 = _mm256_mul(v_doc_counts1, _mm256_mul(v_phi1, v_t51));
 
-            // t6 = doc->counts[n] * ( t2 *  t5);
-            __m256fp v_doc_likelihood = _mm256_mul(v_doc_counts, _mm256_mul(v_phi, v_t5));
+            v_likelihood1 = _mm256_add(v_likelihood1, v_doc_likelihood1);
 
-            // likelihood = likelihood + t6;
-            v_likelihood = _mm256_add(v_likelihood, v_doc_likelihood);
+            // Tile 2
+            __m256fp v_phi2 = _mm256_loadu(phi + (n+2) * model->num_topics + k);
+            __m256fp v_log_phi2 = _mm256_log(v_phi2); 
+            v_log_phi2 = _mm256_max(v_log_phi2, LOW);
+            __m256fp v_l_p_w_doc2 = _mm256_loadu(model->log_prob_w_doc + (n+2) * model->num_topics + k);
+            __m256fp v_t52 = _mm256_add(_mm256_sub(v_dig, v_log_phi2), v_l_p_w_doc2);
+            __m256fp v_doc_likelihood2 = _mm256_mul(v_doc_counts2, _mm256_mul(v_phi2, v_t52));
+
+            v_likelihood2 = _mm256_add(v_likelihood2, v_doc_likelihood2);
+
+            // Tile 3
+            __m256fp v_phi3 = _mm256_loadu(phi + (n+3) * model->num_topics + k);
+            __m256fp v_log_phi3 = _mm256_log(v_phi3); 
+            v_log_phi3 = _mm256_max(v_log_phi3, LOW);
+            __m256fp v_l_p_w_doc3 = _mm256_loadu(model->log_prob_w_doc + (n+3) * model->num_topics + k);
+            __m256fp v_t53 = _mm256_add(_mm256_sub(v_dig, v_log_phi3), v_l_p_w_doc3);
+            __m256fp v_doc_likelihood3 = _mm256_mul(v_doc_counts3, _mm256_mul(v_phi3, v_t53));
+
+            v_likelihood3 = _mm256_add(v_likelihood3, v_doc_likelihood3);
+        }
+        if (LEFTOVER(model->num_topics, 0)) {
+            __m256fp v_dig = _mm256_maskload(dig + k, leftover_mask);
+
+            // Tile 0
+            __m256fp v_phi0 = _mm256_maskload(phi + (n+0) * model->num_topics + k, leftover_mask);
+            __m256fp v_log_phi0 = _mm256_log(v_phi0);
+            v_log_phi0 = _mm256_max(v_log_phi0, LOW);
+            __m256fp v_l_p_w_doc0 = _mm256_maskload(model->log_prob_w_doc + (n+0) * model->num_topics + k, leftover_mask);
+            __m256fp v_t50 = _mm256_add(_mm256_sub(v_dig, v_log_phi0), v_l_p_w_doc0);
+            __m256fp v_doc_likelihood0 = _mm256_mul(v_doc_counts0, _mm256_mul(v_phi0, v_t50));
+
+            v_likelihood0 = _mm256_add(v_likelihood0, v_doc_likelihood0);
+
+            // Tile 1
+            __m256fp v_phi1 = _mm256_maskload(phi + (n+1) * model->num_topics + k, leftover_mask);
+            __m256fp v_log_phi1 = _mm256_log(v_phi1);
+            v_log_phi1 = _mm256_max(v_log_phi1, LOW);
+            __m256fp v_l_p_w_doc1 = _mm256_maskload(model->log_prob_w_doc + (n+1) * model->num_topics + k, leftover_mask);
+            __m256fp v_t51 = _mm256_add(_mm256_sub(v_dig, v_log_phi1), v_l_p_w_doc1);
+            __m256fp v_doc_likelihood1 = _mm256_mul(v_doc_counts1, _mm256_mul(v_phi1, v_t51));
+
+            v_likelihood1 = _mm256_add(v_likelihood1, v_doc_likelihood1);
+
+
+            // Tile 2
+            __m256fp v_phi2 = _mm256_maskload(phi + (n+2) * model->num_topics + k, leftover_mask);
+            __m256fp v_log_phi2 = _mm256_log(v_phi2);
+            v_log_phi2 = _mm256_max(v_log_phi2, LOW);
+            __m256fp v_l_p_w_doc2 = _mm256_maskload(model->log_prob_w_doc + (n+2) * model->num_topics + k, leftover_mask);
+            __m256fp v_t52 = _mm256_add(_mm256_sub(v_dig, v_log_phi2), v_l_p_w_doc2);
+            __m256fp v_doc_likelihood2 = _mm256_mul(v_doc_counts2, _mm256_mul(v_phi2, v_t52));
+
+            v_likelihood2 = _mm256_add(v_likelihood2, v_doc_likelihood2);
+
+            // Tile 3
+            __m256fp v_phi3 = _mm256_maskload(phi + (n+3) * model->num_topics + k, leftover_mask);
+            __m256fp v_log_phi3 = _mm256_log(v_phi3);
+            v_log_phi3 = _mm256_max(v_log_phi3, LOW);
+            __m256fp v_l_p_w_doc3 = _mm256_maskload(model->log_prob_w_doc + (n+3) * model->num_topics + k, leftover_mask);
+            __m256fp v_t53 = _mm256_add(_mm256_sub(v_dig, v_log_phi3), v_l_p_w_doc3);
+            __m256fp v_doc_likelihood3 = _mm256_mul(v_doc_counts3, _mm256_mul(v_phi3, v_t53));
+
+            v_likelihood3 = _mm256_add(v_likelihood3, v_doc_likelihood3);
         }
     }
 
-    __m256fp likelihood_totals = hsum(v_likelihood);
+    // add the four likelihood vectors up so they don't stick around in register
+    v_likelihood0 = _mm256_add(v_likelihood0, v_likelihood1);
+    v_likelihood0 = _mm256_add(v_likelihood0, v_likelihood2);
+    v_likelihood0 = _mm256_add(v_likelihood0, v_likelihood3);
+
+    for (; n < doc->length; n++)
+    {
+        __m256fp v_doc_counts0 = _mm256_set1(doc->counts[n]);
+        for (k = 0; k < kk; k += STRIDE)
+        {   
+            __m256fp v_dig = _mm256_loadu(dig + k);
+
+            // Add to likelihood0
+            __m256fp v_phi0 = _mm256_loadu(phi + (n+0) * model->num_topics + k);
+            __m256fp v_log_phi0 = _mm256_log(v_phi0);
+            v_log_phi0 = _mm256_max(v_log_phi0, LOW);
+            __m256fp v_l_p_w_doc0 = _mm256_loadu(model->log_prob_w_doc + (n+0) * model->num_topics + k);
+            __m256fp v_t50 = _mm256_add(_mm256_sub(v_dig, v_log_phi0), v_l_p_w_doc0);
+            __m256fp v_doc_likelihood0 = _mm256_mul(v_doc_counts0, _mm256_mul(v_phi0, v_t50));
+
+            v_likelihood0 = _mm256_add(v_likelihood0, v_doc_likelihood0);
+        }
+        if (LEFTOVER(model->num_topics, 0)) {
+            __m256fp v_dig = _mm256_maskload(dig + k, leftover_mask);
+
+            // Add to likelihood0
+            __m256fp v_phi0 = _mm256_maskload(phi + (n+0) * model->num_topics + k, leftover_mask);
+            __m256fp v_log_phi0 = _mm256_log(v_phi0);
+            v_log_phi0 = _mm256_max(v_log_phi0, LOW);
+            __m256fp v_l_p_w_doc0 = _mm256_maskload(model->log_prob_w_doc + (n+0) * model->num_topics + k, leftover_mask);
+            __m256fp v_t50 = _mm256_add(_mm256_sub(v_dig, v_log_phi0), v_l_p_w_doc0);
+            __m256fp v_doc_likelihood0 = _mm256_mul(v_doc_counts0, _mm256_mul(v_phi0, v_t50));
+
+            v_likelihood0 = _mm256_add(v_likelihood0, v_doc_likelihood0);
+        }
+
+    } 
+
+    __m256fp likelihood_totals = hsum(v_likelihood0);
     // NOTE += and not = because some part of likelihood is scalar computed
     likelihood += first(likelihood_totals);
+    // printf("%f\n", likelihood);
 
     stop_timer(rdtsc);
 
     return likelihood;
-}
+}
\ No newline at end of file
diff --git a/fast-lda/lda-model.c b/fast-lda/lda-model.c
index 4130ee5..6eaedf9 100644
--- a/fast-lda/lda-model.c
+++ b/fast-lda/lda-model.c
@@ -25,6 +25,7 @@
 #include "fp.h"
 #include "lda-model.h"
 #include "rdtsc-helper.h"
+#include <assert.h>
 
 
 void lda_mle(lda_model* model, lda_suffstats* ss, int estimate_alpha)
@@ -33,28 +34,101 @@ void lda_mle(lda_model* model, lda_suffstats* ss, int estimate_alpha)
 
     timer t = start_timer(MLE);
 
-    for (w = 0; w < model->num_terms; w++)
-    {
-        int kk;
+    int tiling_factor = 4;
+
+    // <FL> Instead of using an if, we just do the log. If we had a zero
+    // we'll get -INF, and this max operation will get rid of it.
+    __m256fp l = _mm256_set1(-100);
+
+    int kk;
         __m256i rem;
         STRIDE_SPLIT(model->num_topics, 0, &kk, &rem);
 
+    for (w = 0; w + tiling_factor - 1 < model->num_terms; w+=tiling_factor)
+    {
+        
         for (k = 0; k < kk; k += STRIDE)
         {
-            __m256fp cw = _mm256_load(ss->class_word + (w * model->num_topics + k));
-            __m256fp ct = _mm256_load(ss->class_total + k);
+            __m256fp ct = _mm256_loadu(ss->class_total + k);
+            __m256fp lct = _mm256_log(ct);
+            
+            // Tile 1
+            __m256fp cw1 = _mm256_loadu(ss->class_word + (w * model->num_topics + k));
+            __m256fp lcw1 = _mm256_log(cw1);
+            __m256fp r1 = _mm256_sub(lcw1, lct);
+            __m256fp f1 = _mm256_max(r1, l);
+            _mm256_storeu(model->log_prob_w + (w * model->num_topics + k), f1);
+
+            // Tile 2
+            __m256fp cw2 = _mm256_loadu(ss->class_word + ((w + 1) * model->num_topics + k));
+            __m256fp lcw2 = _mm256_log(cw2);
+            __m256fp r2 = _mm256_sub(lcw2, lct);
+            __m256fp f2 = _mm256_max(r2, l);
+            _mm256_storeu(model->log_prob_w + ((w + 1) * model->num_topics + k), f2);
+
+            // Tile 3
+            __m256fp cw3 = _mm256_loadu(ss->class_word + ((w + 2) * model->num_topics + k));
+            __m256fp lcw3 = _mm256_log(cw3);
+            __m256fp r3 = _mm256_sub(lcw3, lct);
+            __m256fp f3 = _mm256_max(r3, l);
+            _mm256_storeu(model->log_prob_w + ((w + 2) * model->num_topics + k), f3);
+
+            // Tile 4
+            __m256fp cw4 = _mm256_loadu(ss->class_word + ((w + 3) * model->num_topics + k));
+            __m256fp lcw4 = _mm256_log(cw4);
+            __m256fp r4 = _mm256_sub(lcw4, lct);
+            __m256fp f4 = _mm256_max(r4, l);
+            _mm256_storeu(model->log_prob_w + ((w + 3) * model->num_topics + k), f4);
+        }
+
+        if (LEFTOVER(model->num_topics, 0)) {
+            __m256fp ct = _mm256_maskload(ss->class_total + kk, rem);
+            __m256fp lct = _mm256_log(ct);
+
+            // Tile 1
+            __m256fp cw1 = _mm256_maskload(ss->class_word + (w * model->num_topics + kk), rem);
+            __m256fp lcw1 = _mm256_log(cw1);
+            __m256fp r1 = _mm256_sub(lcw1, lct);
+            __m256fp f1 = _mm256_max(r1, l);
+            _mm256_maskstore(model->log_prob_w + (w * model->num_topics + k), rem, f1);
+
+            // Tile 2
+            __m256fp cw2 = _mm256_loadu(ss->class_word + ((w + 1) * model->num_topics + k));
+            __m256fp lcw2 = _mm256_log(cw2);
+            __m256fp r2 = _mm256_sub(lcw2, lct);
+            __m256fp f2 = _mm256_max(r2, l);
+            _mm256_maskstore(model->log_prob_w + ((w + 1)* model->num_topics + k), rem, f2);
+
+            // // Tile 3
+            __m256fp cw3 = _mm256_loadu(ss->class_word + ((w + 2) * model->num_topics + k));
+            __m256fp lcw3 = _mm256_log(cw3);
+            __m256fp r3 = _mm256_sub(lcw3, lct);
+            __m256fp f3 = _mm256_max(r3, l);
+            _mm256_maskstore(model->log_prob_w + ((w + 2) * model->num_topics + k), rem, f3);
+
+            // Tile 4
+            __m256fp cw4 = _mm256_loadu(ss->class_word + ((w + 3) * model->num_topics + k));
+            __m256fp lcw4 = _mm256_log(cw4);
+            __m256fp r4 = _mm256_sub(lcw4, lct);
+            __m256fp f4 = _mm256_max(r4, l);
+            _mm256_maskstore(model->log_prob_w + ((w + 3) * model->num_topics + k), rem, f4);
+        }
+    }
+
+    for (; w < model->num_terms; w++)
+    {
+        for (k = 0; k < kk; k += STRIDE)
+        {
+            __m256fp cw = _mm256_loadu(ss->class_word + (w * model->num_topics + k));
+            __m256fp ct = _mm256_loadu(ss->class_total + k);
 
             __m256fp lcw = _mm256_log(cw);
             __m256fp lct = _mm256_log(ct);
 
             __m256fp r = _mm256_sub(lcw, lct);
-
-            // <FL> Instead of using an if, we just do the log. If we had a zero
-            // we'll get -INF, and this max operation will get rid of it.
-            __m256fp l = _mm256_set1(-100);
             __m256fp f = _mm256_max(r, l);
 
-            _mm256_store(model->log_prob_w + (w * model->num_topics + k), f);
+            _mm256_storeu(model->log_prob_w + (w * model->num_topics + k), f);
         }
 
         if (LEFTOVER(model->num_topics, 0)) {
@@ -65,21 +139,17 @@ void lda_mle(lda_model* model, lda_suffstats* ss, int estimate_alpha)
             __m256fp lct = _mm256_log(ct);
 
             __m256fp r = _mm256_sub(lcw, lct);
-
-            __m256fp l = _mm256_set1(-100);
             __m256fp f = _mm256_max(r, l);
 
             _mm256_maskstore(model->log_prob_w + (w * model->num_topics + k), rem, f);
         }
     }
 
-
     if (estimate_alpha == 1)
     {
         model->alpha = opt_alpha(ss->alpha_suffstats,
            ss->num_docs,
-           model->num_topics);
-
+           model->num_topics);  
         printf("new alpha = %5.5f\n", model->alpha);
     }
 
@@ -91,11 +161,11 @@ lda_model* new_lda_model(int num_terms, int num_topics, int max_doc_length)
     int i,j;
     lda_model* model;
 
-    model = malloc(sizeof(lda_model));
+    model = _mm_malloc(sizeof(lda_model), ALIGNMENT);
     model->num_topics = num_topics;
     model->num_terms = num_terms;
     model->alpha = 1.0;
-    model->log_prob_w = _mm_malloc(sizeof(fp_t) * num_terms * num_topics, 32);
+    model->log_prob_w = _mm_malloc(sizeof(fp_t) * num_terms * num_topics, ALIGNMENT);
     for (i = 0; i < num_terms; i++)
     {
         for (j = 0; j < num_topics; j++)
@@ -103,7 +173,7 @@ lda_model* new_lda_model(int num_terms, int num_topics, int max_doc_length)
     }
 
     // <CC> Create matrix for log_prob_w for one doc for optimization no 2.
-    model->log_prob_w_doc = _mm_malloc(sizeof(fp_t) * max_doc_length * num_topics, 32);
+    model->log_prob_w_doc = _mm_malloc(sizeof(fp_t) * max_doc_length * num_topics, ALIGNMENT);
     for (i = 0; i < max_doc_length; i++)
     {
         for (j = 0; j < num_topics; j++)
@@ -186,14 +256,16 @@ lda_suffstats* new_lda_suffstats(lda_model* model)
     int num_topics = model->num_topics;
     int num_terms = model->num_terms;
 
-    lda_suffstats* ss = malloc(sizeof(lda_suffstats));
-    ss->class_total = _mm_malloc(num_topics * sizeof(fp_t), 32);
-    for (int i = 0 ; i < num_topics ; i++)
-        ss->class_total[i] = 0;
+    lda_suffstats* ss = _mm_malloc(sizeof(lda_suffstats), ALIGNMENT);
+    ss->class_total = _mm_malloc(num_topics * sizeof(fp_t), ALIGNMENT);
+    ss->class_word = _mm_malloc(num_terms * num_topics * sizeof(fp_t), ALIGNMENT);
 
-    ss->class_word = _mm_malloc(num_terms * num_topics * sizeof(fp_t), 32);
-    for (int i = 0 ; i < num_topics * num_terms ; i++)
+    for (int i = 0; i < num_topics; i++) {
+        ss->class_total[i] = 0;
+    }
+    for (int i = 0; i < num_terms * num_topics; i++) {
         ss->class_word[i] = 0;
+     }
 
     printf("%d\n", num_terms * num_topics);
 
@@ -233,4 +305,4 @@ void random_initialize_ss(lda_suffstats* ss, lda_model* model)
             ss->class_total[k] += ss->class_word[n * num_topics + k];
         }
     }
-}
+}
\ No newline at end of file
diff --git a/fast-lda/lda-run.c b/fast-lda/lda-run.c
index 69d9430..08cf2e4 100644
--- a/fast-lda/lda-run.c
+++ b/fast-lda/lda-run.c
@@ -20,8 +20,6 @@ int main(int argc, char* argv[])
     // seedMT(t1);
     seedMT(4357U);
 
-    char cmd_mkdir[500];
-
     if (argc > 8)
     {
         if (strcmp(argv[1], "est")==0)
@@ -31,18 +29,15 @@ int main(int argc, char* argv[])
             NTOPICS = atoi(argv[4]);
             read_settings(argv[5]);
             corpus = read_data(argv[6], doc_limit);
-            sprintf(cmd_mkdir, "mkdir %s", argv[8]);
-            system(cmd_mkdir);
 
             init_timing_infrastructure();
             run_em(argv[7], argv[8], corpus);
 
             FILE* f;
-            if (argc == 10 && strcmp(argv[9], "-out") == 0) {
+            if (strcmp(argv[9], "-out") == 0) {
                 f = stdout;
             } else {
-                system("mkdir results");
-                f = fopen("results/timings.csv","w");
+                f = fopen(argv[9], "w");
             }
 
             print_timings(f);
@@ -85,7 +80,7 @@ corpus* read_data(char* data_filename, int doc_limit)
     corpus* c;
 
     printf("reading data from %s\n", data_filename);
-    c = malloc(sizeof(corpus));
+    c = _mm_malloc(sizeof(corpus), ALIGNMENT);
     c->docs = 0;
     c->num_terms = 0;
     c->num_docs = 0;
@@ -98,8 +93,8 @@ corpus* read_data(char* data_filename, int doc_limit)
         c->docs = (document*) realloc(c->docs, sizeof(document)*(nd+1));
         c->docs[nd].length = length;
         c->docs[nd].total = 0;
-        c->docs[nd].words = _mm_malloc(sizeof(int)*length, 32);
-        c->docs[nd].counts = _mm_malloc(sizeof(int)*length, 32);
+        c->docs[nd].words = _mm_malloc(sizeof(int)*length, ALIGNMENT);
+        c->docs[nd].counts = _mm_malloc(sizeof(int)*length, ALIGNMENT);
         for (n = 0; n < length; n++)
         {
             fscanf(fileptr, "%10d:%10d", &word, &count);
diff --git a/fast-lda/rdtsc-helper.c b/fast-lda/rdtsc-helper.c
index 97e1ae3..6563d20 100644
--- a/fast-lda/rdtsc-helper.c
+++ b/fast-lda/rdtsc-helper.c
@@ -20,12 +20,19 @@ static char* accumulator_names[] = {
 timer start_timer(int id){
     timer t;
     t.id = id;
-    // CPUID();
+
+    // This CPUID thing seems to do nothing, but is required so that the CPU
+    // doesn't schedule the RDTSC out-of-order.
+    CPUID();
     RDTSC(t.start);
     return t;
 }
 
-void stop_timer(timer t) {
+void stop_timer(timer t){
+
+    // This CPUID thing seems to do nothing, but is required so that the CPU
+    // doesn't schedule the RDTSC out-of-order.
+    CPUID();
     RDTSC(t.end);
     t.cycles = (long long) ((COUNTER_DIFF(t.end, t.start)));
     timing_infrastructure[t.id].sum += t.cycles;
diff --git a/fast-lda/utils.c b/fast-lda/utils.c
index fa36948..fdabb8e 100644
--- a/fast-lda/utils.c
+++ b/fast-lda/utils.c
@@ -1,14 +1,4 @@
 #include "utils.h"
-#include "rdtsc-helper.h"
-
-#if defined(NO_MKL) || !defined(__INTEL_COMPILER)
-    void vdLGamma(int stride, const fp_t* input, fp_t* output){
-        for(int i=0;i<stride;i++)
-        {
-            output[i] = lgamma(input[i]);
-        }
-    }
-#endif //defined(NO_MKL) || !defined(__INTEL_COMPILER)
 
 int argmax(fp_t* x, int n)
 {
diff --git a/fast-lda/utils.h b/fast-lda/utils.h
index 5b556f6..a6f1314 100644
--- a/fast-lda/utils.h
+++ b/fast-lda/utils.h
@@ -20,18 +20,27 @@
         #define INLINE inline
     #endif // __INTEL_COMPILER
 #else
+    // Only tell the compiler we'd like this to be inlined.
+    // Note: icc seems to be more eager to inline than gcc
     #define INLINE inline
 #endif // FORCE_INLINE
 
+
+// This is not really a performance-critical function.
+int argmax(fp_t* x, int n);
+
 #if defined(NO_MKL) || !defined(__INTEL_COMPILER)
-    void vdLGamma(int stride, const fp_t* input, fp_t* output);
+    INLINE
+    void vdLGamma(int stride, const fp_t* input, fp_t* output){
+        for(int i=0;i<stride;i++)
+        {
+            output[i] = lgamma(input[i]);
+        }
+    }
 #else
     #include "mkl.h"
 #endif //defined(NO_MKL) || !defined(__INTEL_COMPILER)
 
-// This is not really a performance-critical function.
-int argmax(fp_t* x, int n);
-
 
 #ifndef __INTEL_COMPILER
     /* These are some replacements for the icc-specific intrinsics. The
@@ -40,14 +49,13 @@ int argmax(fp_t* x, int n);
 
     INLINE
     __m256fp _mm256_log(__m256fp x) {
-        // This is pretty terrible but it's basically the only thing we can do.
-        fp_t vals[STRIDE];
-        _mm256_storeu(vals, x);
+        fp_t* vals = (fp_t*) &x;
 
+        // This is pretty terrible but it's basically the only thing we can do.
         for (int i = 0 ; i < STRIDE ; i++)
             vals[i] = (fp_t) log(vals[i]);
 
-        return _mm256_loadu(vals);
+        return x;
     }
 
     INLINE
@@ -71,7 +79,6 @@ int argmax(fp_t* x, int n);
 
         return x;
     }
-
 #endif // __INTEL_COMPILER
 
 
@@ -79,6 +86,7 @@ int argmax(fp_t* x, int n);
 // ====================================================
 
 /* given log(a) and log(b), return log(a + b) */
+
 INLINE
 fp_t log_sum(fp_t log_a, fp_t log_b)
 {
@@ -133,20 +141,32 @@ fp_t trigamma(fp_t x)
 INLINE
 fp_t digamma(fp_t x)
 {
+    // (??+, ?*, 8/, 1 log) 126 cycles
     timer rdtsc = start_timer(DIGAMMA);
 
-    fp_t p;
-    x=x+6;
-    p=1/(x*x);
-    p=(((0.004166666666667*p-0.003968253986254)*p+
-	 0.008333333333333)*p-0.083333333333333)*p;
-    p=p+log(x)-0.5/x-1/(x-1)-1/(x-2)-1/(x-3)-1/(x-4)-1/(x-5)-1/(x-6);
+    fp_t p, p_sq, z, t0, t1, t2, t3, t4, t5;
+    z=x + 6;
+    p=1 / (z*z);
+    p_sq = p*p;
+
+    const fp_t a6 = + 0.2531135531135531,
+               a5 = - 0.007575757575757576,
+               a4 = + 0.004166666666667,
+               a3 = - 0.003968253986254,
+               a2 = + 0.008333333333333,
+               a1 = - 0.08333333333333;
+
+    t4 = a4 * p_sq + a3 * p;
+    t3 = p_sq * t4;
+    t2 = a2 * p_sq + a1 * p;
+    t1 = t2 + t3;
+    t0 = log(z) - 0.5/z - 1/(z-1) -1/(z-2) -1/(z-3) -1/(z-4) -1/(z-5) -1/(z-6);
+    p = t1 + t0;
 
     stop_timer(rdtsc);
     return p;
 }
 
-
 INLINE
 fp_t log_gamma(fp_t x)
 {
@@ -168,6 +188,85 @@ fp_t log_gamma(fp_t x)
 // ======== Vector-argument math functions ========
 // ================================================
 
+// INLINE
+// __m256fp approx_log1p(__m256fp x)
+// {
+//     __m256fp a1 = _mm256_set1(0.99949556);
+//     __m256fp a2 = _mm256_set1(-0.49190896);
+//     __m256fp a3 = _mm256_set1(0.28947478);
+//     __m256fp a4 = _mm256_set1(-0.13606275);
+//     __m256fp a5 = _mm256_set1(0.03215845);
+    
+//     __m256fp x_squared = _mm256_mul(x, x);
+//     __m256fp x_4 = _mm256_mul(x_squared, x_squared);
+
+//     __m256fp t1 = _mm256_mul(a1, x);
+//     __m256fp t2 = _mm256_mul(a2, x_squared);
+//     __m256fp t3 = _mm256_mul(a3, _mm256_mul(x_squared, x));
+//     __m256fp t4 = _mm256_mul(a4, x_4);
+//     __m256fp t5 = _mm256_mul(a5, _mm256_mul(x_4, x));
+            
+//     t1 = _mm256_add(t1, t2);
+//     t1 = _mm256_add(t1, t3);
+//     t1 = _mm256_add(t1, t4);
+//     t1 = _mm256_add(t1, t5);
+
+//     return t1;
+// }
+
+// INLINE
+// __m256fp approx_log1p(__m256fp x)
+// {
+//     // eight terms 
+//     __m256fp a1 = _mm256_set1(0.9999964239);
+//     __m256fp a2 = _mm256_set1(-0.4998741238);
+//     __m256fp a3 = _mm256_set1(0.3317990258);
+//     __m256fp a4 = _mm256_set1(-0.2407338084);
+//     __m256fp a5 = _mm256_set1(0.1676540711);
+//     __m256fp a6 = _mm256_set1(-0.0953293897);
+//     __m256fp a7 = _mm256_set1(0.0380684937);
+//     __m256fp a8 = _mm256_set1(-0.0064535442);
+    
+//     __m256fp x_squared = _mm256_mul(x, x);
+//     __m256fp x_4 = _mm256_mul(x_squared, x_squared);
+//     __m256fp x_6 = _mm256_mul(x_4, x_squared);
+
+
+//     __m256fp t1 = _mm256_mul(a1, x);
+//     __m256fp t2 = _mm256_mul(a2, x_squared);
+//     __m256fp t3 = _mm256_mul(a3, _mm256_mul(x_squared, x));
+//     __m256fp t4 = _mm256_mul(a4, x_4);
+//     __m256fp t5 = _mm256_mul(a5, _mm256_mul(x_4, x));
+//     __m256fp t6 = _mm256_mul(a6, x_6);
+//     __m256fp t7 = _mm256_mul(a7, _mm256_mul(x_6, x));
+//     __m256fp t8 = _mm256_mul(a8, _mm256_mul(x_4, x_4));
+            
+//     t1 = _mm256_add(t1, t2);
+//     t1 = _mm256_add(t1, t3);
+//     t1 = _mm256_add(t1, t4);
+//     t1 = _mm256_add(t1, t5);
+//     t1 = _mm256_add(t1, t6);
+//     t1 = _mm256_add(t1, t7);
+//     t1 = _mm256_add(t1, t8);
+
+//     return t1;
+// }
+
+// INLINE
+// __m256fp looped_log1p(__m256fp x)
+// {
+//     fp_t input[4];
+//     _mm256_storeu(input, x);
+//     for(int i=0;i<4;i++)
+//     {
+//         input[i] = log1p(input[i]);
+//     }
+
+//     __m256fp output = _mm256_loadu(input);
+//     return output;
+// }
+
+
 /* given log(a) and log(b), return log(a + b) */
 INLINE
 __m256fp log_sum_vec(__m256fp log_a, __m256fp log_b)
@@ -175,23 +274,24 @@ __m256fp log_sum_vec(__m256fp log_a, __m256fp log_b)
    // v = log_b+log(1 + exp(log_a-log_b));
   timer rdtsc = start_timer(LOG_SUM);
 
-  __m256fp ones = _mm256_set1(1);
+  // used for straightforward impl. __m256fp ones = _mm256_set1(1);
   __m256fp res = _mm256_sub(log_a, log_b);
   res = _mm256_exp(res);
-  res = _mm256_add(ones, res);
-  res = _mm256_log(res);
+  //  used for straightforward impl. res = _mm256_add(ones, res);
+  res = _mm256_log1p(res);
+  //  used for straightforward impl. res = _mm256_log(res);
   res = _mm256_add(log_b, res);
 
   stop_timer(rdtsc);
   return res;
 }
 
-INLINE
+INLINE 
 __m256fp log_sum_vec_masked(__m256fp log_a, __m256fp log_b, __m256i mask)
 {
     __m256fp v_log_sum = log_sum_vec(log_a, log_b);
     __m256fp v_mask = _mm256_castsi256(mask);
-    // We need to chose if we had a log to add or the entry was masked and then
+    // We need to chose if we had a log to add or the entry was masked and then 
     // we just take the log_a which was computed already, because we cannot do
     // log(x + 0).
     __m256fp res = _mm256_blendv(log_a, v_log_sum, v_mask);
@@ -201,80 +301,74 @@ __m256fp log_sum_vec_masked(__m256fp log_a, __m256fp log_b, __m256i mask)
 
 INLINE
 __m256fp digamma_vec(__m256fp x)
-{
+{   // 190 cycles :(
     timer rdtsc = start_timer(DIGAMMA);
 
+    __m256fp SIXES = _mm256_set1(6);
 
+    // z = x + 6;
+    __m256fp z  = _mm256_add(x, SIXES);
+    // p = 1 / (z*z)
+    __m256fp zsq = _mm256_mul(z, z);
+    __m256fp p   = _mm256_rcp(zsq);
+    // p_sq = p*p;
+    __m256fp p_sq = _mm256_mul(p, p);
+
+
+    // const fp_t a4 = ...
+    __m256fp a4 = _mm256_set1((fp_t) + 0.004166666666667);
+    __m256fp a3 = _mm256_set1((fp_t) - 0.003968253986254);
+    __m256fp a2 = _mm256_set1((fp_t) + 0.008333333333333);
+    __m256fp a1 = _mm256_set1((fp_t) - 0.08333333333333);
+
+    //t4 = a4 * p_sq + a3 * p;
+    __m256fp a4psq = _mm256_mul(a4, p_sq);
+    __m256fp a3p = _mm256_mul(a3, p);
+    __m256fp t4 = _mm256_add(a4psq, a3p);
+    // t3 = p_sq * t4;
+    __m256fp t3 = _mm256_mul(p_sq, t4);
+    // t2 = a2 * p_sq + a1 * p;
+    __m256fp a2psq = _mm256_mul(a2, p_sq);
+    __m256fp a1p = _mm256_mul(a1, p);
+    __m256fp t2 = _mm256_add(a2psq, a1p);
+    // t1 = t2 + t3;
+    __m256fp t1 = _mm256_add(t2, t3);
+
+    // t0 = log(z) - 0.5/z - 1/(z-1) -1/(z-2) -1/(z-3) -1/(z-4) -1/(z-5) -1/(z-6);
     __m256fp HALVES = _mm256_set1(0.5);
     __m256fp ONES = _mm256_set1(1);
     __m256fp TWOS = _mm256_set1(2);
     __m256fp THREES = _mm256_set1(3);
     __m256fp FOURS = _mm256_set1(4);
     __m256fp FIVES = _mm256_set1(5);
-    __m256fp SIXES = _mm256_set1(6);
-    __m256fp ONE_120TH = _mm256_set1((fp_t) 0.008333333333333);
-    __m256fp ONE_240TH = _mm256_set1((fp_t) 0.004166666666667);
-    // <FL> I have no idea what this is. Google returns other implementations of
-    // the digamma function or things like "the epic floating point battle".
-    __m256fp DIGAMMA_CONST = _mm256_set1((fp_t) 0.003968253986254);
 
-    // x = x + 6
-    __m256fp x6  = _mm256_add(x, SIXES);
-
-    // p = 1 / (x*x)
-    __m256fp xsq = _mm256_mul(x6, x6);
-    __m256fp p   = _mm256_rcp(xsq);
-
-    /* <FL>
-     * Use fp associativity.
-     * We go from (((p/240 - const)p + 1/120)p - 1/240)p
-     * to (p/240 - const) p^3 + p^2/120 - p/240
-     * This strikes a balance between making the tree shallower and increasing
-     * the number of flops. The first version takes 28 cycles, while this one
-     * takes 20. The main gain comes from p/240 appearing twice, as well as p^2.
-     */
-    __m256fp p240 = _mm256_mul(p, ONE_240TH);
-    __m256fp psq = _mm256_mul(p, p);
-
-    __m256fp psq120 = _mm256_mul(psq, ONE_120TH);
-    __m256fp pcu = _mm256_mul(psq, p);
-    __m256fp p240c = _mm256_sub(p240, DIGAMMA_CONST);
-
-    // No ILP here
-    __m256fp r = _mm256_mul(p240c, pcu);
-    __m256fp q = _mm256_add(r, psq120);
-    p = _mm256_sub(q, p240);
-
-
-    // p+log(x)-0.5/x-1/(x-1)-1/(x-2)-1/(x-3)-1/(x-4)-1/(x-5)-1/(x-6)
-    // Tons of ILP here
-    __m256fp logx = _mm256_log(x6);
-    __m256fp hox = _rcp_const(HALVES, x6);
-
-    __m256fp xm1 = _mm256_sub(x6, ONES);
-    __m256fp xm2 = _mm256_sub(x6, TWOS);
-    __m256fp xm3 = _mm256_sub(x6, THREES);
-    __m256fp xm4 = _mm256_sub(x6, FOURS);
-    __m256fp xm5 = _mm256_sub(x6, FIVES);
-    __m256fp xm6 = _mm256_sub(x6, SIXES);
-
-    __m256fp xm1r = _mm256_rcp(xm1);
-    __m256fp xm2r = _mm256_rcp(xm2);
-    __m256fp xm3r = _mm256_rcp(xm3);
-    __m256fp xm4r = _mm256_rcp(xm4);
-    __m256fp xm5r = _mm256_rcp(xm5);
-    __m256fp xm6r = _mm256_rcp(xm6);
+    __m256fp logz = _mm256_log(z);
+    __m256fp hoz = _rcp_const(HALVES, z);
+
+    __m256fp zm1 = _mm256_sub(z, ONES);
+    __m256fp zm1r = _mm256_rcp(zm1);
+    __m256fp zm2 = _mm256_sub(z, TWOS);
+    __m256fp zm2r = _mm256_rcp(zm2);
+    __m256fp zm12r = _mm256_add(zm1r, zm2r);
+    __m256fp logz_m_hoz = _mm256_sub(logz, hoz);
+    __m256fp a = _mm256_sub(logz_m_hoz, zm12r);
+    __m256fp zm3 = _mm256_sub(z, THREES);
+    __m256fp zm3r = _mm256_rcp(zm3);
+    __m256fp zm4 = _mm256_sub(z, FOURS);
+    __m256fp zm4r = _mm256_rcp(zm4);
+    __m256fp zm34r = _mm256_add(zm3r, zm4r);
+    __m256fp zm5 = _mm256_sub(z, FIVES);
+    __m256fp zm5r = _mm256_rcp(zm5);
+    __m256fp zm6 = _mm256_sub(z, SIXES);
+    __m256fp zm6r = _mm256_rcp(zm6);
+    __m256fp zm56r = _mm256_add(zm5r, zm6r);
+    __m256fp b = _mm256_add(zm34r, zm56r);
 
     // Maximally exploit associativity
-    __m256fp logx_m_hox = _mm256_sub(logx, hox);
-    __m256fp xm12r = _mm256_add(xm1r, xm2r);
-    __m256fp xm34r = _mm256_add(xm3r, xm4r);
-    __m256fp xm56r = _mm256_add(xm5r, xm6r);
+    __m256fp t0 = _mm256_sub(a, b);
 
-    __m256fp a = _mm256_sub(logx_m_hox, xm12r);
-    __m256fp b = _mm256_add(xm34r, xm56r);
-    __m256fp c = _mm256_sub(a, b);
-    __m256fp result = _mm256_add(p, c);
+    // p = t1 + t0;
+    __m256fp result = _mm256_add(t1, t0);
 
     stop_timer(rdtsc);
     return result;
@@ -358,8 +452,10 @@ __m256fp log_gamma_vec(__m256fp x)
     return t;
 }
 
-// First element of this vector
-#define first(x) *((fp_t*) &(x))
+
+// ======== Vector-argument math functions ========
+// ================================================
+
 
 // hsum(x): return a vector where all elements are set to the sum of elements of x.
 #ifdef FLOAT
@@ -394,7 +490,9 @@ __m256fp log_gamma_vec(__m256fp x)
         // -> [A..D * 4]
         return _mm256_hadd_pd(x, x);
     }
-#endif // FLOAT
+#endif // DOUBLE
 
+// First element of this vector
+#define first(x) *((fp_t*) &(x))
 
 #endif // UTILS_H
diff --git a/notes/OptimizationsLog.txt b/notes/OptimizationsLog.txt
index 96da32f..231530b 100644
--- a/notes/OptimizationsLog.txt
+++ b/notes/OptimizationsLog.txt
@@ -10,7 +10,7 @@ Optimizations
 	Thus if we transpose the matrices to be [V][K] then for one work we load in the cacheline
 	values for 8 topics which gains 7 hits.
 
-	Given that this happens for each document we can compare the secnarios given 
+	Given that this happens for each document we can compare the secnarios given
 	that the document length is D:
 		model->log_prob_w accesses: K * D (compute_likelihood)
 									K * D (lda_inference)
@@ -25,14 +25,14 @@ Optimizations
 
 	1) [K][V] miss ratio = 100%
 
-	2) [V][K] miss ratio ~ 1/8 = 12.5%	 
+	2) [V][K] miss ratio ~ 1/8 = 12.5%
 
 	Actual improvements: TO DO
 
-2. TO DO: Save the matrices model->log_prob_w[K][V] and ss->class_word[K][V] for one document
+
+2. DONE: Save the matrices model->log_prob_w[K][V] and ss->class_word[K][V] for one document
 	for the reiteration because we jump through 10000 words and we might replace things
 	in the L1 cache which we could avoid.
-
 	Instead gather the rowns needed for one doc_e_step and scatter them after the computation.
 
 	Calculations show we save ~ 1/16 STLB/TLB? misses(write down the calculations)
@@ -49,8 +49,9 @@ Optimizations
 4. MAYBE DO: In one inference loop, go in chunks of columns. This way some
     values of vectors size k can be replaced by a scalar
 
-3. MAYBE DO: In the inference, do the convergence loop not for the entire matrix log_prob_w (or the smaller matrix for one document),
-				but do stuff until convergence for smaller parts (rowwise of the matrix). This would include also adapting the compute_likelihood method somehow so it will process chunks instead of the whole matrix.
 
-4. MAYBE DO: In one inference loop, go in chunks of columns. This way some values of vectors size k can be replaced by a scalar
+5. TO DO: Multiple accumulators in loops of
+    doc_e_step (first loop updating suffstats and gamma_sum)
+    compute_likelihood (first two loops)
+
 
diff --git a/notes/flop counts.txt b/notes/flop counts.txt
index 97876a6..2d43375 100644
--- a/notes/flop counts.txt	
+++ b/notes/flop counts.txt	
@@ -8,7 +8,7 @@ Where;
 LDA_MLE
 
     N * K * (
-        2       (1-, 1+)
+        2       (1-, 1>)
       + 2       (log)
     ) 
     + OPT_ALPHA
diff --git a/roofline.py b/roofline.py
index c6a3645..8c82ae9 100644
--- a/roofline.py
+++ b/roofline.py
@@ -1,7 +1,6 @@
 import matplotlib.pyplot as plt
-from benchmarking import run_em
-from benchmarking import read_one_output
-import benchmarking
+import pltutils
+from pltutils import fns
 import sys
 import re
 import os
@@ -9,8 +8,9 @@ from os.path import join
 
 
 X_MAX_LIM = 2**(9)
-X_MIN_LIM = 2**(-8)
-Y_MIN_LIM = 2**(-8)
+X_MIN_LIM = 2**(-4)
+Y_MIN_LIM = 2**(-4)
+Y_MAX_LIM = 2**(6)
 
 class Run:
     def __init__(self, opints, perfs, nums_docs, label):
@@ -45,10 +45,10 @@ def make_axes(axes):
     axes.set_axisbelow(True)
     axes.yaxis.grid(color='white', linestyle='solid')
     axes.set_facecolor((211.0/255,211.0/255,211.0/255))
-    axes.set_ylim(X_MIN_LIM, X_MAX_LIM)
+    axes.set_ylim(X_MIN_LIM, Y_MAX_LIM)
     axes.set_xlim(X_MIN_LIM, X_MAX_LIM)
 
-    plt.ylabel('Performance [GFlops/s]',rotation="0")
+    plt.ylabel('Performance [flops/cycle]',rotation="0")
     axes.yaxis.set_label_coords(0.09,1.02)
     axes.spines['left'].set_color('#dddddd')
     axes.spines['right'].set_color('#dddddd')
@@ -58,10 +58,10 @@ def make_axes(axes):
 def plot_roofs(axes, precision):
     # Pi_no_vec, pi_vec
     if (precision == 'd'):
-        pi = [2, 8]
+        pi = [2, 16]
         names = ['$_{scalar}$', '$_{vector}$']
     elif (precision == 'f'):
-        pi = [2, 16]
+        pi = [2, 32]
         names = ['$_{scalar}$', '$_{vector}$']
     else:
         print("I do not know what precision you are talking about.\n")
@@ -122,14 +122,14 @@ def plot_run(run, col):
         size='x-small',
         arrowprops=dict(arrowstyle = '-'))
 
-    xlab = run.opints[0]
-    ylab = max(run.perfs) + 0.5 * max(run.perfs)
-
-
-    plt.text(xlab, ylab, run.label, color=col, ha='center', size='x-small')
+    # Label is to the right of the first element, since in general we go
+    # left as we go further in the series
+    xlab = run.opints[0] + 2
+    ylab = run.perfs[0]
+    plt.text(xlab, ylab, run.label, color=col, ha='left', va='center', size='x-small')
 
 def parse_perf_files(dir_path):
-    benchmarking.set_corpus_stats(dir_path)
+    pltutils.set_corpus_stats(dir_path)
 
     operational_intensity = []
     memory_reads = []
@@ -171,32 +171,34 @@ def parse_perf_files(dir_path):
     num_docs, bytes_transfers = (zip(*sorted(zip(num_docs, bytes_transfers))))
 
     #Get the flop count and the performance from the timings
-    data = {}
+    data = {'x' : [], 'y' : []}
     for filename in os.listdir(dir_path):
         if("timings" in filename):
-            f = open(join(dir_path, filename), "r")
+            fullname = join(dir_path, filename)
             # Extract K and N from the filename
             K, N = map(int, re.findall(regex, filename))
             if (K, N) in num_docs:
-                read_one_output(f, N, K, data)
-                flop_count[ num_docs.index((K, N)) ] = run_em(N, K).full()
+                k1, n1, flops, _, perf = pltutils.read_one_output(fullname)
+                assert k1 == K and n1 == N, "Wrong file"
+                flop_count[ num_docs.index((K, N)) ] = flops[ fns.index("RUN_EM") ]
+                data['x'].append(N)
+                data['y'].append(perf[ fns.index("RUN_EM") ])
                 #print(flop_count)
                 pass
             pass
         pass
 
-    print('')
-
     operational_intensity = [x / y for x, y in zip(flop_count, bytes_transfers)]
 
     plt_op = []
     plt_perf = []
     for i in range(len(num_docs)):
         n = num_docs[i][1]
-        if n in data['RUN_EM']['x']:
+        if n in data['x']:
+            idx = data['x'].index(n)
             assert flop_count[i] is not 0
             plt_op.append(operational_intensity[i])
-            plt_perf.append(data['RUN_EM']['y'][ data['RUN_EM']['x'].index(n) ])
+            plt_perf.append(data['y'][idx])
 
     return Run(plt_op, plt_perf, num_docs, comment)
 
diff --git a/runner.py b/runner.py
index daee400..3afc501 100644
--- a/runner.py
+++ b/runner.py
@@ -15,7 +15,7 @@ REFERENCE_DATA = REFERENCE_FOLDER + '/ref-%d-%d-%s-%s.beta'
 LDA_EXE_LOG = './%s-lda/logfiles'
 LDA_OUT_BETA = LDA_EXE_LOG + '/final.beta'
 LDA_RESULTS = './results'
-LDA_OUT_TIMING = LDA_RESULTS + '/timings.csv'
+LDA_OUT_TIMING = LDA_RESULTS + '/timings_%s.csv'
 
 ALWAYS_GENERATE_REF = False
 ALWAYS_USE_REF = False
@@ -76,15 +76,16 @@ def set_mkl_env():
 def make_lda_params(which, k, n):
     if which == 'fast':
       set_mkl_env()
-    return ['./%s-lda/lda' % which,           # Executable location
-            'est',                              # Execution mode (always est)
-            str(n),                             # Number of documents
-            '1',                                # Initial estimate for alpha
-            str(k),                             # Number of topics
-            'master-settings.txt',              # Settings location
-            CORPUS % which,       # Documents location
-            'random',                           # Initialization method (only random)
-            LDA_EXE_LOG % which]                # Output directory
+    return ['./%s-lda/lda' % which,         # Executable location
+            'est',                          # Execution mode (always est)
+            str(n),                         # Number of documents
+            '1',                            # Initial estimate for alpha
+            str(k),                         # Number of topics
+            'master-settings.txt',          # Settings location
+            CORPUS % which,                 # Documents location
+            'random',                       # Initialization method (only random)
+            LDA_EXE_LOG % which,            # Output directory (lda)
+            LDA_OUT_TIMING % RUN_NAME]
 
 def exists(path):
     try:
@@ -184,7 +185,7 @@ def bench(k, n, which):
             run_cmd(make_lda_params(lda, k, n))
 
         timing_out = (TIMING_FOLDER % RUN_NAME) + (TIMING_FILENAME % (lda, k, n))
-        os.rename(LDA_OUT_TIMING, timing_out)
+        os.rename(LDA_OUT_TIMING % RUN_NAME, timing_out)
 
 def record_vitals(comment, options, cmdline):
     os.makedirs(TIMING_FOLDER % RUN_NAME)
diff --git a/slow-lda/Makefile b/slow-lda/Makefile
index 7e468fe..4c5408b 100644
--- a/slow-lda/Makefile
+++ b/slow-lda/Makefile
@@ -25,7 +25,7 @@ OUTNAME=-olda
 # CC should be either gcc (default), icc (intel linux) or icl (intel windows)
 #_CRT_SECURE_NO_WARNINGS: silence warnings about scanf, fopen and other stuff.
 ifeq ($(CC),gcc)
-	CFLAGS= -O3 -Wall -g -std=c99 -D_CRT_SECURE_NO_WARNINGS -march=core-avx2 -fno-tree-vectorize
+	CFLAGS= -O3 -Wall -g -std=c99 -D_CRT_SECURE_NO_WARNINGS -march=core-avx2 -fno-vectorize -fno-tree-vectorize -fno-slp-vectorize
 	LDFLAGS= -lm
 else
 	ifeq ($(CC),icl)
diff --git a/slow-lda/lda-data.c b/slow-lda/lda-data.c
index 935cc46..a5f1f33 100644
--- a/slow-lda/lda-data.c
+++ b/slow-lda/lda-data.c
@@ -27,7 +27,7 @@ corpus* read_data(char* data_filename, int doc_limit)
     corpus* c;
 
     printf("reading data from %s\n", data_filename);
-    c = malloc(sizeof(corpus));
+    c = _mm_malloc(sizeof(corpus), ALIGNMENT);
     c->docs = 0;
     c->num_terms = 0;
     c->num_docs = 0;
@@ -40,8 +40,8 @@ corpus* read_data(char* data_filename, int doc_limit)
         c->docs = (document*) realloc(c->docs, sizeof(document)*(nd+1));
         c->docs[nd].length = length;
         c->docs[nd].total = 0;
-        c->docs[nd].words = malloc(sizeof(int)*length);
-        c->docs[nd].counts = malloc(sizeof(int)*length);
+        c->docs[nd].words = _mm_malloc(sizeof(int)*length, ALIGNMENT);
+        c->docs[nd].counts = _mm_malloc(sizeof(int)*length, ALIGNMENT);
         for (n = 0; n < length; n++)
         {
             fscanf(fileptr, "%10d:%10d", &word, &count);
diff --git a/slow-lda/lda-estimate.c b/slow-lda/lda-estimate.c
index d7fe82b..2d6dd5c 100644
--- a/slow-lda/lda-estimate.c
+++ b/slow-lda/lda-estimate.c
@@ -153,9 +153,7 @@ void run_em(char* start, char* directory, corpus* corpus)
         model = load_lda_model(start);
         ss = new_lda_suffstats(model);
     }
-
-    sprintf(filename,"%s/000",directory);
-    save_lda_model(model, filename);
+    
 
     // run expectation maximization
 
@@ -309,7 +307,6 @@ int main(int argc, char* argv[])
             NTOPICS = atoi(argv[4]);
             read_settings(argv[5]);
             corpus = read_data(argv[6], doc_limit);
-            make_directory(argv[8]);
             run_em(argv[7], argv[8], corpus);
         }
         if (strcmp(argv[1], "inf")==0)
@@ -326,11 +323,10 @@ int main(int argc, char* argv[])
     }
 
     FILE* f;
-    if (argc == 10 && strcmp(argv[9], "-out") == 0) {
+    if (strcmp(argv[9], "-out") == 0) {
         f = stdout;
     } else {
-        system("mkdir results");
-        f = fopen("results/timings.csv","w");
+        f = fopen(argv[9],"w");
     }
 
     print_timings(f);
diff --git a/slow-lda/utils.c b/slow-lda/utils.c
index 9e860a2..0bafd89 100644
--- a/slow-lda/utils.c
+++ b/slow-lda/utils.c
@@ -98,20 +98,6 @@ fp_t log_gamma(fp_t x)
 }
 
 
-
-/*
- * make directory
- *
- */
-
-void make_directory(char* name)
-{
-    char cmd[500] = {0};
-    sprintf(cmd, "mkdir %s", name);
-    system(cmd);
-}
-
-
 /*
  * argmax
  *
diff --git a/slow-lda/utils.h b/slow-lda/utils.h
index b49f6db..68b6bd9 100644
--- a/slow-lda/utils.h
+++ b/slow-lda/utils.h
@@ -14,7 +14,6 @@ fp_t log_sum(fp_t log_a, fp_t log_b);
 fp_t trigamma(fp_t x);
 fp_t digamma(fp_t x);
 fp_t log_gamma(fp_t x);
-void make_directory(char* name);
 int argmax(fp_t* x, int n);
 
 #endif
diff --git a/timings/FINAL_STAGE42_gcc_small/fast_timings_24_2246.csv b/timings/FINAL_STAGE42_gcc_small/fast_timings_24_2246.csv
deleted file mode 100644
index 133ab5d..0000000
--- a/timings/FINAL_STAGE42_gcc_small/fast_timings_24_2246.csv
+++ /dev/null
@@ -1,14 +0,0 @@
-Accumulator, Total count, Average count
-RUN_EM, 1776454120536, 1776454120536.000000
-LDA_INFERENCE, 1771172377384, 17922492.283089
-DIGAMMA, 391329991000, 374.623286
-LOG_SUM, 637752124882, 461.992313
-LOG_GAMMA, The code inside this timer has not been called, 0
-TRIGAMMA, 45318, 39.613636
-DOC_E_STEP, 1774393836028, 17955090.221282
-LIKELIHOOD, 423410053580, 349712.657315
-MLE, 1992156962, 44270154.711111
-OPT_ALPHA, 577270, 13119.772727
-EM_CONVERGE, 44, 44.000000
-INFERENCE_CONVERGE, 1210737, 12.251447
-ALPHA_CONVERGE, 572, 13.000000
diff --git a/timings/FINAL_STAGE42_gcc_small/fast_timings_36_2246.csv b/timings/FINAL_STAGE42_gcc_small/fast_timings_36_2246.csv
deleted file mode 100644
index b6a44a7..0000000
--- a/timings/FINAL_STAGE42_gcc_small/fast_timings_36_2246.csv
+++ /dev/null
@@ -1,14 +0,0 @@
-Accumulator, Total count, Average count
-RUN_EM, 2435963482452, 2435963482452.000000
-LDA_INFERENCE, 2428561520670, 25146115.271283
-DIGAMMA, 538024502454, 373.105347
-LOG_SUM, 895248485782, 512.291335
-LOG_GAMMA, The code inside this timer has not been called, 0
-TRIGAMMA, 44014, 39.795660
-DOC_E_STEP, 2432935785476, 25191407.830727
-LIKELIHOOD, 577886508540, 518259.229418
-MLE, 2926087246, 66501982.863636
-OPT_ALPHA, 583470, 13569.069767
-EM_CONVERGE, 43, 43.000000
-INFERENCE_CONVERGE, 1115053, 11.545621
-ALPHA_CONVERGE, 553, 12.860465
diff --git a/timings/FINAL_STAGE42_gcc_small/fast_timings_48_2246.csv b/timings/FINAL_STAGE42_gcc_small/fast_timings_48_2246.csv
deleted file mode 100644
index d0a725b..0000000
--- a/timings/FINAL_STAGE42_gcc_small/fast_timings_48_2246.csv
+++ /dev/null
@@ -1,14 +0,0 @@
-Accumulator, Total count, Average count
-RUN_EM, 3650555373648, 3650555373648.000000
-LDA_INFERENCE, 3639164258136, 31159362.440373
-DIGAMMA, 803222166350, 370.038517
-LOG_SUM, 1355553260636, 539.750251
-LOG_GAMMA, The code inside this timer has not been called, 0
-TRIGAMMA, 53486, 39.974589
-DOC_E_STEP, 3645539952288, 31213952.601959
-LIKELIHOOD, 859987411486, 684163.539346
-MLE, 4711515870, 88896525.849057
-OPT_ALPHA, 674224, 12965.846154
-EM_CONVERGE, 52, 52.000000
-INFERENCE_CONVERGE, 1256991, 10.762646
-ALPHA_CONVERGE, 669, 12.865385
diff --git a/timings/FINAL_STAGE42_gcc_small/fast_timings_60_2246.csv b/timings/FINAL_STAGE42_gcc_small/fast_timings_60_2246.csv
deleted file mode 100644
index 7a1407e..0000000
--- a/timings/FINAL_STAGE42_gcc_small/fast_timings_60_2246.csv
+++ /dev/null
@@ -1,14 +0,0 @@
-Accumulator, Total count, Average count
-RUN_EM, 4225052337946, 4225052337946.000000
-LDA_INFERENCE, 4211118269578, 37498826.977542
-DIGAMMA, 931609830032, 370.380215
-LOG_SUM, 1581945580570, 559.524493
-LOG_GAMMA, The code inside this timer has not been called, 0
-TRIGAMMA, 50194, 39.710443
-DOC_E_STEP, 4219198238854, 37570776.837524
-LIKELIHOOD, 996861236876, 853398.176261
-MLE, 5632815798, 110447368.588235
-OPT_ALPHA, 651588, 13031.760000
-EM_CONVERGE, 50, 50.000000
-INFERENCE_CONVERGE, 1168108, 10.401674
-ALPHA_CONVERGE, 632, 12.640000
diff --git a/timings/FINAL_STAGE42_gcc_small/info.txt b/timings/FINAL_STAGE42_gcc_small/info.txt
deleted file mode 100644
index a306a1b..0000000
--- a/timings/FINAL_STAGE42_gcc_small/info.txt
+++ /dev/null
@@ -1,93 +0,0 @@
-Bench run 2017-05-26_18-41-23
-Ran with the command-line `bench --k=24,60,12 -fag`
-Comment: "Stage 4.2 small dataset"
-===============================
-
-OPTIONS
-use_icc=False
-use_long=False
-use_doubles=True
-END OPTIONS
-
-Latest commit at time of writing:
-a3ee20ee (on branch Stage4.2-VecInline)
-
-Additionally, the repo contained the following untracked files:
-	timings/2017-05-26_15-55-15/.~lock.fast_timings_400_10.csv#
-	timings/2017-05-26_15-55-15/fast_timings_100_10.csv
-	timings/2017-05-26_15-55-15/fast_timings_200_10.csv
-	timings/2017-05-26_15-55-15/fast_timings_300_10.csv
-	timings/2017-05-26_15-55-15/fast_timings_400_10.csv
-	timings/2017-05-26_15-55-15/info.txt
-	timings/2017-05-26_15-55-15/perf_fast_100_10.txt
-	timings/2017-05-26_15-55-15/perf_fast_200_10.txt
-	timings/2017-05-26_15-55-15/perf_fast_300_10.txt
-	timings/2017-05-26_15-55-15/perf_fast_400_10.txt
-	timings/2017-05-26_15-55-15/stage3-align.tar.gz
-	timings/2017-05-26_16-49-05/info.txt
-	timings/2017-05-26_16-49-05/perf_fast_100_10.txt
-	timings/2017-05-26_16-58-08/info.txt
-	timings/2017-05-26_16-58-08/perf_fast_100_10.txt
-	timings/2017-05-26_17-19-36/fast_timings_100_10.csv
-	timings/2017-05-26_17-19-36/fast_timings_200_10.csv
-	timings/2017-05-26_17-19-36/fast_timings_300_10.csv
-	timings/2017-05-26_17-19-36/fast_timings_400_10.csv
-	timings/2017-05-26_17-19-36/final_timings.tar.gz
-	timings/2017-05-26_17-19-36/info.txt
-	timings/2017-05-26_17-19-36/perf_fast_100_10.txt
-	timings/2017-05-26_17-19-36/perf_fast_200_10.txt
-	timings/2017-05-26_17-19-36/perf_fast_300_10.txt
-	timings/2017-05-26_17-19-36/perf_fast_400_10.txt
-	timings/2017-05-26_18-15-30/info.txt
-	timings/2017-05-26_18-15-30/perf_fast_24_2246.txt
-	timings/2017-05-26_18-19-13/info.txt
-	timings/2017-05-26_18-19-13/perf_fast_24_2246.txt
-	timings/2017-05-26_18-41-23/info.txt
-
-The diff, if any, follows:
-
-diff --git a/fast-lda/lda-inference.c b/fast-lda/lda-inference.c
-index 8b12976..ccbe01a 100644
---- a/fast-lda/lda-inference.c
-+++ b/fast-lda/lda-inference.c
-@@ -30,7 +30,7 @@ fp_t lda_inference(document* doc, lda_model* model, fp_t* var_gamma, fp_t* phi)
-     fp_t phisum = 0, likelihood = 0;
-     fp_t likelihood_old = 0, oldphi[model->num_topics];
-     int k, n, var_iter;
--    fp_t digamma_gam[model->num_topics];
-+    fp_t *digamma_gam = _mm_malloc(sizeof(fp_t)*model->num_topics, 32);
- 
-     timer rdtsc = start_timer(LDA_INFERENCE);
- 
-@@ -180,7 +180,7 @@ fp_t lda_inference(document* doc, lda_model* model, fp_t* var_gamma, fp_t* phi)
- fp_t compute_likelihood(document* doc, lda_model* model, fp_t* phi, fp_t* var_gamma)
- {
-     fp_t likelihood = 0, digsum = 0, var_gamma_sum = 0;
--    fp_t dig[model->num_topics];
-+    fp_t *dig = _mm_malloc(sizeof(fp_t)*model->num_topics, 32);
-     __m256fp v_likelihood = _mm256_set1(0),
- 
-     v_var_gamma_sum = _mm256_set1(0);
-diff --git a/slow-lda/Makefile b/slow-lda/Makefile
-index 4c5408b..7e468fe 100644
---- a/slow-lda/Makefile
-+++ b/slow-lda/Makefile
-@@ -25,7 +25,7 @@ OUTNAME=-olda
- # CC should be either gcc (default), icc (intel linux) or icl (intel windows)
- #_CRT_SECURE_NO_WARNINGS: silence warnings about scanf, fopen and other stuff.
- ifeq ($(CC),gcc)
--	CFLAGS= -O3 -Wall -g -std=c99 -D_CRT_SECURE_NO_WARNINGS -march=core-avx2 -fno-vectorize -fno-tree-vectorize -fno-slp-vectorize
-+	CFLAGS= -O3 -Wall -g -std=c99 -D_CRT_SECURE_NO_WARNINGS -march=core-avx2 -fno-tree-vectorize
- 	LDFLAGS= -lm
- else
- 	ifeq ($(CC),icl)
-
-===============================
-
-Settings file:
-var max iter 20
-var convergence 1e-6
-em max iter 100
-em convergence 1e-4
-alpha estimate
-
diff --git a/timings/FINAL_STAGE42_gcc_small/perf_fast_24_2246.txt b/timings/FINAL_STAGE42_gcc_small/perf_fast_24_2246.txt
deleted file mode 100644
index 2b29447..0000000
--- a/timings/FINAL_STAGE42_gcc_small/perf_fast_24_2246.txt
+++ /dev/null
@@ -1,20 +0,0 @@
-mkdir: cannot create directory ./fast-lda/logfiles: File exists
-mkdir: cannot create directory results: File exists
-
- Performance counter stats for './fast-lda/lda est 2246 1 24 master-settings.txt ./fast-lda/ap/ap.dat random ./fast-lda/logfiles':
-
- 3,296,591,177,121      instructions              #    1.60  insn per cycle           (41.67%)
- 2,063,600,481,694      cycles                                                        (50.00%)
-     1,744,816,669      cache-references                                              (50.00%)
-        56,493,544      cache-misses              #    3.238 % of all cache refs      (50.00%)
-       177,139,887      LLC-loads                                                     (50.00%)
-        10,265,412      LLC-load-misses           #    5.80% of all LL-cache hits     (50.00%)
-       102,922,678      LLC-stores                                                    (16.67%)
-         7,204,935      LLC-store-misses                                              (16.67%)
-   749,180,844,794      dTLB-loads                                                    (25.00%)
-         8,322,547      dTLB-load-misses          #    0.00% of all dTLB cache hits   (33.33%)
-   302,650,844,425      dTLB-stores                                                   (33.33%)
-           381,625      dTLB-store-misses                                             (33.33%)
-
-     521.847922840 seconds time elapsed
-
diff --git a/timings/FINAL_STAGE42_gcc_small/perf_fast_36_2246.txt b/timings/FINAL_STAGE42_gcc_small/perf_fast_36_2246.txt
deleted file mode 100644
index 77652d7..0000000
--- a/timings/FINAL_STAGE42_gcc_small/perf_fast_36_2246.txt
+++ /dev/null
@@ -1,20 +0,0 @@
-mkdir: cannot create directory ./fast-lda/logfiles: File exists
-mkdir: cannot create directory results: File exists
-
- Performance counter stats for './fast-lda/lda est 2246 1 36 master-settings.txt ./fast-lda/ap/ap.dat random ./fast-lda/logfiles':
-
- 4,565,604,362,594      instructions              #    1.61  insn per cycle           (41.67%)
- 2,832,549,844,245      cycles                                                        (50.00%)
-     3,036,431,102      cache-references                                              (50.00%)
-        95,206,706      cache-misses              #    3.135 % of all cache refs      (50.00%)
-       314,294,961      LLC-loads                                                     (50.00%)
-        18,892,657      LLC-load-misses           #    6.01% of all LL-cache hits     (50.00%)
-       160,767,092      LLC-stores                                                    (16.67%)
-         9,968,710      LLC-store-misses                                              (16.67%)
- 1,036,063,191,591      dTLB-loads                                                    (25.00%)
-         8,590,345      dTLB-load-misses          #    0.00% of all dTLB cache hits   (33.33%)
-   417,111,692,447      dTLB-stores                                                   (33.33%)
-           736,139      dTLB-store-misses                                             (33.33%)
-
-     715.740385015 seconds time elapsed
-
diff --git a/timings/FINAL_STAGE42_gcc_small/perf_fast_48_2246.txt b/timings/FINAL_STAGE42_gcc_small/perf_fast_48_2246.txt
deleted file mode 100644
index dec7ce1..0000000
--- a/timings/FINAL_STAGE42_gcc_small/perf_fast_48_2246.txt
+++ /dev/null
@@ -1,20 +0,0 @@
-mkdir: cannot create directory ./fast-lda/logfiles: File exists
-mkdir: cannot create directory results: File exists
-
- Performance counter stats for './fast-lda/lda est 2246 1 48 master-settings.txt ./fast-lda/ap/ap.dat random ./fast-lda/logfiles':
-
- 6,880,458,329,604      instructions              #    1.62  insn per cycle           (41.67%)
- 4,251,111,796,706      cycles                                                        (50.00%)
-     5,683,498,438      cache-references                                              (50.00%)
-       130,773,949      cache-misses              #    2.301 % of all cache refs      (50.00%)
-       566,957,965      LLC-loads                                                     (50.00%)
-        27,139,001      LLC-load-misses           #    4.79% of all LL-cache hits     (50.00%)
-       251,257,816      LLC-stores                                                    (16.67%)
-        12,968,512      LLC-store-misses                                              (16.67%)
- 1,560,221,419,285      dTLB-loads                                                    (25.00%)
-        12,431,436      dTLB-load-misses          #    0.00% of all dTLB cache hits   (33.33%)
-   627,581,412,980      dTLB-stores                                                   (33.33%)
-           595,151      dTLB-store-misses                                             (33.33%)
-
-    1072.012007584 seconds time elapsed
-
diff --git a/timings/FINAL_STAGE42_gcc_small/perf_fast_60_2246.txt b/timings/FINAL_STAGE42_gcc_small/perf_fast_60_2246.txt
deleted file mode 100644
index 79b6e6a..0000000
--- a/timings/FINAL_STAGE42_gcc_small/perf_fast_60_2246.txt
+++ /dev/null
@@ -1,20 +0,0 @@
-mkdir: cannot create directory ./fast-lda/logfiles: File exists
-mkdir: cannot create directory results: File exists
-
- Performance counter stats for './fast-lda/lda est 2246 1 60 master-settings.txt ./fast-lda/ap/ap.dat random ./fast-lda/logfiles':
-
- 7,984,096,384,415      instructions              #    1.63  insn per cycle           (41.67%)
- 4,912,530,283,713      cycles                                                        (50.00%)
-     9,913,576,770      cache-references                                              (50.00%)
-       215,315,154      cache-misses              #    2.172 % of all cache refs      (50.00%)
-       899,082,823      LLC-loads                                                     (50.00%)
-        50,951,193      LLC-load-misses           #    5.67% of all LL-cache hits     (50.00%)
-       314,190,640      LLC-stores                                                    (16.67%)
-        18,872,722      LLC-store-misses                                              (16.67%)
- 1,809,784,116,113      dTLB-loads                                                    (25.00%)
-        16,695,899      dTLB-load-misses          #    0.00% of all dTLB cache hits   (33.33%)
-   727,205,607,260      dTLB-stores                                                   (33.33%)
-           712,238      dTLB-store-misses                                             (33.33%)
-
-    1241.010796972 seconds time elapsed
-
diff --git a/todo.txt b/todo.txt
index a64eddb..c090ac3 100644
--- a/todo.txt
+++ b/todo.txt
@@ -1,8 +1,3 @@
-2) Generate smaller reference outputs from the various parameter ranges decided upon
-
-6) Script to run the timings for different input size and plot performance.
-7) Update project descriptions:
-		point 4 - decribe validation
-		point 7 - performance plot decription
-
-8) Script for Automatic flop, memeory transfers counters which uses the caller/callee DAG. 
\ No newline at end of file
+- Run Slow, Stage1, Stage2 again with the --non-vec option
+- Run ICC vs GCC comparison
+- Update weighting of different types of Flops

===============================

Settings file:
var max iter 20
var convergence 1e-6
em max iter 100
em convergence 1e-4
alpha estimate

